{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Versions\n",
    "\n",
    "numpy==1.26.3\n",
    "\n",
    "pandas==2.1.4\n",
    "\n",
    "nltk==3.8.1\n",
    "\n",
    "seaborn==0.13.1\n",
    "\n",
    "bs4==0.0.1\n",
    "\n",
    "scikit-learn==1.3.2\n",
    "\n",
    "torch==2.1.2\n",
    "\n",
    "torchaudio==2.1.2\n",
    "\n",
    "torchvision==0.16.2\n",
    "\n",
    "gensim==4.3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "import seaborn as sns\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_df = pd.read_csv(\n",
    "    \"amazon_reviews_us_Office_Products_v1_00.tsv\",\n",
    "    sep=\"\\t\",\n",
    "    on_bad_lines=\"skip\",\n",
    "    low_memory=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = original_df[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>marketplace</th>\n",
       "      <th>customer_id</th>\n",
       "      <th>review_id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>product_parent</th>\n",
       "      <th>product_title</th>\n",
       "      <th>product_category</th>\n",
       "      <th>star_rating</th>\n",
       "      <th>helpful_votes</th>\n",
       "      <th>total_votes</th>\n",
       "      <th>vine</th>\n",
       "      <th>verified_purchase</th>\n",
       "      <th>review_headline</th>\n",
       "      <th>review_body</th>\n",
       "      <th>review_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>US</td>\n",
       "      <td>43081963</td>\n",
       "      <td>R18RVCKGH1SSI9</td>\n",
       "      <td>B001BM2MAC</td>\n",
       "      <td>307809868</td>\n",
       "      <td>Scotch Cushion Wrap 7961, 12 Inches x 100 Feet</td>\n",
       "      <td>Office Products</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>Five Stars</td>\n",
       "      <td>Great product.</td>\n",
       "      <td>2015-08-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>US</td>\n",
       "      <td>10951564</td>\n",
       "      <td>R3L4L6LW1PUOFY</td>\n",
       "      <td>B00DZYEXPQ</td>\n",
       "      <td>75004341</td>\n",
       "      <td>Dust-Off Compressed Gas Duster, Pack of 4</td>\n",
       "      <td>Office Products</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>Phffffffft, Phfffffft. Lots of air, and it's C...</td>\n",
       "      <td>What's to say about this commodity item except...</td>\n",
       "      <td>2015-08-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>US</td>\n",
       "      <td>21143145</td>\n",
       "      <td>R2J8AWXWTDX2TF</td>\n",
       "      <td>B00RTMUHDW</td>\n",
       "      <td>529689027</td>\n",
       "      <td>Amram Tagger Standard Tag Attaching Tagging Gu...</td>\n",
       "      <td>Office Products</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>but I am sure I will like it.</td>\n",
       "      <td>Haven't used yet, but I am sure I will like it.</td>\n",
       "      <td>2015-08-31</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  marketplace  customer_id       review_id  product_id  product_parent  \\\n",
       "0          US     43081963  R18RVCKGH1SSI9  B001BM2MAC       307809868   \n",
       "1          US     10951564  R3L4L6LW1PUOFY  B00DZYEXPQ        75004341   \n",
       "2          US     21143145  R2J8AWXWTDX2TF  B00RTMUHDW       529689027   \n",
       "\n",
       "                                       product_title product_category  \\\n",
       "0     Scotch Cushion Wrap 7961, 12 Inches x 100 Feet  Office Products   \n",
       "1          Dust-Off Compressed Gas Duster, Pack of 4  Office Products   \n",
       "2  Amram Tagger Standard Tag Attaching Tagging Gu...  Office Products   \n",
       "\n",
       "  star_rating  helpful_votes  total_votes vine verified_purchase  \\\n",
       "0           5            0.0          0.0    N                 Y   \n",
       "1           5            0.0          1.0    N                 Y   \n",
       "2           5            0.0          0.0    N                 Y   \n",
       "\n",
       "                                     review_headline  \\\n",
       "0                                         Five Stars   \n",
       "1  Phffffffft, Phfffffft. Lots of air, and it's C...   \n",
       "2                      but I am sure I will like it.   \n",
       "\n",
       "                                         review_body review_date  \n",
       "0                                     Great product.  2015-08-31  \n",
       "1  What's to say about this commodity item except...  2015-08-31  \n",
       "2    Haven't used yet, but I am sure I will like it.  2015-08-31  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(subset=\"review_headline\")\n",
    "df = df.dropna(subset=\"review_body\")\n",
    "\n",
    "df[\"review_text\"] = df[\"review_headline\"] + \" \" + df[\"review_body\"]\n",
    "\n",
    "columns = [\"star_rating\", \"review_text\"]\n",
    "\n",
    "df = df[columns]\n",
    "\n",
    "df.loc[:, (\"star_rating\")] = pd.to_numeric(df[\"star_rating\"], errors='coerce')\n",
    "\n",
    "df = df.dropna(subset=\"star_rating\")\n",
    "df = df.dropna(subset=\"review_text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>star_rating</th>\n",
       "      <th>review_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>Five Stars Great product.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>Phffffffft, Phfffffft. Lots of air, and it's C...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>but I am sure I will like it. Haven't used yet...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  star_rating                                        review_text\n",
       "0           5                          Five Stars Great product.\n",
       "1           5  Phffffffft, Phfffffft. Lots of air, and it's C...\n",
       "2           5  but I am sure I will like it. Haven't used yet..."
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>star_rating</th>\n",
       "      <th>review_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>and the shredder was dirty and the bin was par...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>One Star worked about a month then died</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>One Star The phone did not work.  No Dial Tone...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>One Star Not laminated and no reinforced holes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>One Star Cartridge was over filled, black smea...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  star_rating                                        review_text\n",
       "0           1  and the shredder was dirty and the bin was par...\n",
       "1           1            One Star worked about a month then died\n",
       "2           1  One Star The phone did not work.  No Dial Tone...\n",
       "3           1  One Star Not laminated and no reinforced holes...\n",
       "4           1  One Star Cartridge was over filled, black smea..."
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples_per_rating = 50000\n",
    "# samples_per_rating = 50\n",
    "\n",
    "df1 = df.loc[df[\"star_rating\"] == 1]\n",
    "df1 = df1[:samples_per_rating]\n",
    "\n",
    "df2 = df.loc[df[\"star_rating\"] == 2]\n",
    "df2 = df2[:samples_per_rating]\n",
    "\n",
    "df3 = df.loc[df[\"star_rating\"] == 3]\n",
    "df3 = df3[:samples_per_rating]\n",
    "\n",
    "df4 = df.loc[df[\"star_rating\"] == 4]\n",
    "df4 = df4[:samples_per_rating]\n",
    "\n",
    "df5 = df.loc[df[\"star_rating\"] == 5]\n",
    "df5 = df5[:samples_per_rating]\n",
    "\n",
    "df = pd.concat([df1, df2, df3, df4, df5], ignore_index=True)\n",
    "df = df.sort_index()\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"sentiment_score\"] = 3\n",
    "\n",
    "df.loc[df[\"star_rating\"] > 3, \"sentiment_score\"] = 2\n",
    "df.loc[df[\"star_rating\"] < 3, \"sentiment_score\"] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>star_rating</th>\n",
       "      <th>review_text</th>\n",
       "      <th>sentiment_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>and the shredder was dirty and the bin was par...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>One Star worked about a month then died</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>One Star The phone did not work.  No Dial Tone...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>One Star Not laminated and no reinforced holes...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>One Star Cartridge was over filled, black smea...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  star_rating                                        review_text  \\\n",
       "0           1  and the shredder was dirty and the bin was par...   \n",
       "1           1            One Star worked about a month then died   \n",
       "2           1  One Star The phone did not work.  No Dial Tone...   \n",
       "3           1  One Star Not laminated and no reinforced holes...   \n",
       "4           1  One Star Cartridge was over filled, black smea...   \n",
       "\n",
       "   sentiment_score  \n",
       "0                1  \n",
       "1                1  \n",
       "2                1  \n",
       "3                1  \n",
       "4                1  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/g9/r7hfr6xs6gd6cg8z55mvvqnh0000gn/T/ipykernel_13272/2697138477.py:6: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  lambda x: BeautifulSoup(x).get_text()\n"
     ]
    }
   ],
   "source": [
    "# Transform string to lower case\n",
    "df[\"review_text\"] = df[\"review_text\"].str.lower()\n",
    "\n",
    "# Remove html tags\n",
    "df[\"review_text\"] = df[\"review_text\"].apply(\n",
    "    lambda x: BeautifulSoup(x).get_text()\n",
    ")\n",
    "\n",
    "# Remove urls\n",
    "df[\"review_text\"] = df[\"review_text\"].apply(\n",
    "    lambda x: re.sub('http[s]?://\\S+', '', x)\n",
    ")\n",
    "\n",
    "# remove any non-alphabetic charcters\n",
    "df[\"review_text\"] = df[\"review_text\"].apply(\n",
    "    lambda x: re.sub(r'[^a-z\\s]+', '', x)\n",
    ")\n",
    "\n",
    "# remove any extra spaces\n",
    "df[\"review_text\"] = df[\"review_text\"].apply(\n",
    "    lambda x: re.sub(r' +', ' ', x)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "contractions = {\n",
    "    re.compile(r\"\\bain't\\b\", re.I | re.U): \"are not\",\n",
    "    re.compile(r\"\\ba'ight\\b\", re.I | re.U): \"alright\",\n",
    "    re.compile(r\"\\bamn't\\b\", re.I | re.U): \"am not\",\n",
    "    re.compile(r\"\\bn\\b\", re.I | re.U): \"and\",\n",
    "    re.compile(r\"\\barencha\\b\", re.I | re.U): \"are not you\",\n",
    "    re.compile(r\"\\baren't\\b\", re.I | re.U): \"are not\",\n",
    "    re.compile(r\"\\b'bout\\b\", re.I | re.U): \"about\",\n",
    "    re.compile(r\"\\bcan't\\b\", re.I | re.U): \"cannot\",\n",
    "    re.compile(r\"\\bcap'n\\b\", re.I | re.U): \"captain\",\n",
    "    re.compile(r\"\\bcause\\b\", re.I | re.U): \"because\",\n",
    "    re.compile(r\"\\bcuz\\b\", re.I | re.U): \"because\",\n",
    "    re.compile(r\"\\bcept\\b\", re.I | re.U): \"except\",\n",
    "    re.compile(r\"\\bc'mon\\b\", re.I | re.U): \"come on\",\n",
    "    re.compile(r\"\\bcould've\\b\", re.I | re.U): \"could have\",\n",
    "    re.compile(r\"\\bcouldn't\\b\", re.I | re.U): \"could not\",\n",
    "    re.compile(r\"\\bcouldn't've\\b\", re.I | re.U): \"could not have\",\n",
    "    re.compile(r\"\\bcuppa\\b\", re.I | re.U): \"cup of\",\n",
    "    re.compile(r\"\\bdaren't\\b\", re.I | re.U): \"dare not\",\n",
    "    re.compile(r\"\\bdaresn't\\b\", re.I | re.U): \"dare not\",\n",
    "    re.compile(r\"\\bdasn't\\b\", re.I | re.U): \"dare not\",\n",
    "    re.compile(r\"\\bdidn't\\b\", re.I | re.U): \"did not\",\n",
    "    re.compile(r\"\\bdoesn't\\b\", re.I | re.U): \"does not\",\n",
    "    re.compile(r\"\\bdon't\\b\", re.I | re.U): \"do not\",\n",
    "    re.compile(r\"\\bdunno\\b\", re.I | re.U): \"do not know\",\n",
    "    re.compile(r\"\\bd'ya\\b\", re.I | re.U): \"did you\",\n",
    "    re.compile(r\"\\be'en\\b\", re.I | re.U): \"even\",\n",
    "    re.compile(r\"\\be'er\\b\", re.I | re.U): \"ever\",\n",
    "    re.compile(r\"\\beverybody's\\b\", re.I | re.U): \"everybody is\",\n",
    "    re.compile(r\"\\beveryone's\\b\", re.I | re.U): \"everyone is\",\n",
    "    re.compile(r\"\\beverything's\\b\", re.I | re.U): \"everything is\",\n",
    "    re.compile(r\"\\b'em\\b\", re.I | re.U): \"them\",\n",
    "    re.compile(r\"\\bfinna\\b\", re.I | re.U): \"fixing to\",\n",
    "    re.compile(r\"\\bfo'c'sle\\b\", re.I | re.U): \"forecastle\",\n",
    "    re.compile(r\"\\bgainst\\b\", re.I | re.U): \"against\",\n",
    "    re.compile(r\"\\bg'day\\b\", re.I | re.U): \"good day\",\n",
    "    re.compile(r\"\\bgimme\\b\", re.I | re.U): \"give me\",\n",
    "    re.compile(r\"\\bgiv'n\\b\", re.I | re.U): \"given\",\n",
    "    re.compile(r\"\\bgi'z\\b\", re.I | re.U): \"give us\",\n",
    "    re.compile(r\"\\bgonna\\b\", re.I | re.U): \"going to\",\n",
    "    re.compile(r\"\\bgon't\\b\", re.I | re.U): \"go not\",\n",
    "    re.compile(r\"\\bgotta\\b\", re.I | re.U): \"got to\",\n",
    "    re.compile(r\"\\bhadn't\\b\", re.I | re.U): \"had not\",\n",
    "    re.compile(r\"\\bhad've\\b\", re.I | re.U): \"had have\",\n",
    "    re.compile(r\"\\bhasn't\\b\", re.I | re.U): \"has not\",\n",
    "    re.compile(r\"\\bhaven't\\b\", re.I | re.U): \"have not\",\n",
    "    re.compile(r\"\\bhelluva\\b\", re.I | re.U): \"hell of a\",\n",
    "    re.compile(r\"\\bhe'd\\b\", re.I | re.U): \"he would\",\n",
    "    re.compile(r\"\\bhe'll\\b\", re.I | re.U): \"he will\",\n",
    "    re.compile(r\"\\bhe's\\b\", re.I | re.U): \"he is\",\n",
    "    re.compile(r\"\\byes'nt\\b\", re.I | re.U): \"no\",\n",
    "    re.compile(r\"\\bhow'd\\b\", re.I | re.U): \"how did\",\n",
    "    re.compile(r\"\\bhere's\\b\", re.I | re.U): \"here is\",\n",
    "    re.compile(r\"\\bhowdy\\b\", re.I | re.U): \"how do you do\",\n",
    "    re.compile(r\"\\bhow'll\\b\", re.I | re.U): \"how will\",\n",
    "    re.compile(r\"\\bhow're\\b\", re.I | re.U): \"how are\",\n",
    "    re.compile(r\"\\bI'd've\\b\", re.I | re.U): \"I would have\",\n",
    "    re.compile(r\"\\bI'd'nt\\b\", re.I | re.U): \"I would not\",\n",
    "    re.compile(r\"\\bI'd'nt've\\b\", re.I | re.U): \"I would not have\",\n",
    "    re.compile(r\"\\bIf'n\\b\", re.I | re.U): \"If and when\",\n",
    "    re.compile(r\"\\bI'm\\b\", re.I | re.U): \"I am\",\n",
    "    re.compile(r\"\\bImma\\b\", re.I | re.U): \"I am going to\",\n",
    "    re.compile(r\"\\bI'm'o\\b\", re.I | re.U): \"I am going to\",\n",
    "    re.compile(r\"\\binnit\\b\", re.I | re.U): \"is it not\",\n",
    "    re.compile(r\"\\bIon\\b\", re.I | re.U): \"I do not\",\n",
    "    re.compile(r\"\\bI've\\b\", re.I | re.U): \"I have\",\n",
    "    re.compile(r\"\\bisn't\\b\", re.I | re.U): \"is not\",\n",
    "    re.compile(r\"\\bit'd\\b\", re.I | re.U): \"it would\",\n",
    "    re.compile(r\"\\bIdunno\\b\", re.I | re.U): \"I do not know\",\n",
    "    re.compile(r\"\\bkinda\\b\", re.I | re.U): \"kind of\",\n",
    "    re.compile(r\"\\blet's\\b\", re.I | re.U): \"let us\",\n",
    "    re.compile(r\"\\bloven't\\b\", re.I | re.U): \"love not\",\n",
    "    re.compile(r\"\\bma'am\\b\", re.I | re.U): \"madam\",\n",
    "    re.compile(r\"\\bmayn't\\b\", re.I | re.U): \"may not\",\n",
    "    re.compile(r\"\\bmay've\\b\", re.I | re.U): \"may have\",\n",
    "    re.compile(r\"\\bmethinks\\b\", re.I | re.U): \"I think\",\n",
    "    re.compile(r\"\\bmightn't\\b\", re.I | re.U): \"might not\",\n",
    "    re.compile(r\"\\bmight've\\b\", re.I | re.U): \"might have\",\n",
    "    re.compile(r\"\\bmine's\\b\", re.I | re.U): \"mine is\",\n",
    "    re.compile(r\"\\bmustn't\\b\", re.I | re.U): \"must not\",\n",
    "    re.compile(r\"\\bmustn't've\\b\", re.I | re.U): \"must not have\",\n",
    "    re.compile(r\"\\bmust've\\b\", re.I | re.U): \"must have\",\n",
    "    re.compile(r\"\\b'neath\\b\", re.I | re.U): \"beneath\",\n",
    "    re.compile(r\"\\bneedn't\\b\", re.I | re.U): \"need not\",\n",
    "    re.compile(r\"\\bnal\\b\", re.I | re.U): \"and all\",\n",
    "    re.compile(r\"\\bne'er\\b\", re.I | re.U): \"never\",\n",
    "    re.compile(r\"\\bo'clock\\b\", re.I | re.U): \"of the clock\",\n",
    "    re.compile(r\"\\bo'er\\b\", re.I | re.U): \"over\",\n",
    "    re.compile(r\"\\bol'\\b\", re.I | re.U): \"old\",\n",
    "    re.compile(r\"\\bought've\\b\", re.I | re.U): \"ought have\",\n",
    "    re.compile(r\"\\boughtn't\\b\", re.I | re.U): \"ought not\",\n",
    "    re.compile(r\"\\boughtn't've\\b\", re.I | re.U): \"ought not have\",\n",
    "    re.compile(r\"\\b'round\\b\", re.I | re.U): \"around\",\n",
    "    re.compile(r\"\\b's\\b\", re.I | re.U): \"is\",\n",
    "    re.compile(r\"\\bshalln't\\b\", re.I | re.U): \"shall not\",\n",
    "    re.compile(r\"\\bshan'\\b\", re.I | re.U): \"shall not\",\n",
    "    re.compile(r\"\\bshan't\\b\", re.I | re.U): \"shall not\",\n",
    "    re.compile(r\"\\bshould've\\b\", re.I | re.U): \"should have\",\n",
    "    re.compile(r\"\\bshouldn't\\b\", re.I | re.U): \"should not\",\n",
    "    re.compile(r\"\\bshouldn't've\\b\", re.I | re.U): \"should not have\",\n",
    "    re.compile(r\"\\bso're\\b\", re.I | re.U): \"so are\",\n",
    "    re.compile(r\"\\bso've\\b\", re.I | re.U): \"so have\",\n",
    "    re.compile(r\"\\bthat're\\b\", re.I | re.U): \"that are\",\n",
    "    re.compile(r\"\\bthere're\\b\", re.I | re.U): \"there are\",\n",
    "    re.compile(r\"\\bthese're\\b\", re.I | re.U): \"these are\",\n",
    "    re.compile(r\"\\bthese've\\b\", re.I | re.U): \"these have\",\n",
    "    re.compile(r\"\\bthey've\\b\", re.I | re.U): \"they have\",\n",
    "    re.compile(r\"\\bthose're\\b\", re.I | re.U): \"those are\",\n",
    "    re.compile(r\"\\bthose've\\b\", re.I | re.U): \"those have\",\n",
    "    re.compile(r\"\\b'thout\\b\", re.I | re.U): \"without\",\n",
    "    re.compile(r\"\\b'til\\b\", re.I | re.U): \"until\",\n",
    "    re.compile(r\"\\b'tis\\b\", re.I | re.U): \"it is\",\n",
    "    re.compile(r\"\\bto've\\b\", re.I | re.U): \"to have\",\n",
    "    re.compile(r\"\\btryna\\b\", re.I | re.U): \"trying to\",\n",
    "    re.compile(r\"\\b'twas\\b\", re.I | re.U): \"it was\",\n",
    "    re.compile(r\"\\b'tween\\b\", re.I | re.U): \"between\",\n",
    "    re.compile(r\"\\b'twere\\b\", re.I | re.U): \"it were\",\n",
    "    re.compile(r\"\\bw'all\\b\", re.I | re.U): \"we all\",\n",
    "    re.compile(r\"\\bw'at\\b\", re.I | re.U): \"we at\",\n",
    "    re.compile(r\"\\bwanna\\b\", re.I | re.U): \"want to\",\n",
    "    re.compile(r\"\\bwasn't\\b\", re.I | re.U): \"was not\",\n",
    "    re.compile(r\"\\bwe'd've\\b\", re.I | re.U): \"we would have\",\n",
    "    re.compile(r\"\\bwe're\\b\", re.I | re.U): \"we are\",\n",
    "    re.compile(r\"\\bwe've\\b\", re.I | re.U): \"we have\",\n",
    "    re.compile(r\"\\bweren't\\b\", re.I | re.U): \"were not\",\n",
    "    re.compile(r\"\\bwhatcha\\b\", re.I | re.U): \"what are you\",\n",
    "    re.compile(r\"\\bwhat'd\\b\", re.I | re.U): \"what did\",\n",
    "    re.compile(r\"\\bwhat've\\b\", re.I | re.U): \"what have\",\n",
    "    re.compile(r\"\\bwhen'd\\b\", re.I | re.U): \"when did\",\n",
    "    re.compile(r\"\\bwhere'd\\b\", re.I | re.U): \"where did\",\n",
    "    re.compile(r\"\\bwhere're\\b\", re.I | re.U): \"where are\",\n",
    "    re.compile(r\"\\bwhere've\\b\", re.I | re.U): \"where have\",\n",
    "    re.compile(r\"\\bwhich're\\b\", re.I | re.U): \"which are\",\n",
    "    re.compile(r\"\\bwhich've\\b\", re.I | re.U): \"which have\",\n",
    "    re.compile(r\"\\bwho'd've\\b\", re.I | re.U): \"who would have\",\n",
    "    re.compile(r\"\\bwho're\\b\", re.I | re.U): \"who are\",\n",
    "    re.compile(r\"\\bwho've\\b\", re.I | re.U): \"who have\",\n",
    "    re.compile(r\"\\bwhy'd\\b\", re.I | re.U): \"why did\",\n",
    "    re.compile(r\"\\bwhy're\\b\", re.I | re.U): \"why are\",\n",
    "    re.compile(r\"\\bwilln't\\b\", re.I | re.U): \"will not\",\n",
    "    re.compile(r\"\\bwon't\\b\", re.I | re.U): \"will not\",\n",
    "    re.compile(r\"\\bwonnot\\b\", re.I | re.U): \"will not\",\n",
    "    re.compile(r\"\\bwould've\\b\", re.I | re.U): \"would have\",\n",
    "    re.compile(r\"\\bwouldn't\\b\", re.I | re.U): \"would not\",\n",
    "    re.compile(r\"\\bwouldn't've\\b\", re.I | re.U): \"would not have\",\n",
    "    re.compile(r\"\\by'all\\b\", re.I | re.U): \"you all\",\n",
    "    re.compile(r\"\\by'all'd've\\b\", re.I | re.U): \"you all would have\",\n",
    "    re.compile(r\"\\by'all'dn't've\\b\", re.I | re.U): \"you all would not have\",\n",
    "    re.compile(r\"\\by'all're\\b\", re.I | re.U): \"you all are\",\n",
    "    re.compile(r\"\\by'all'ren't\\b\", re.I | re.U): \"you all are not\",\n",
    "    re.compile(r\"\\by'at\\b\", re.I | re.U): \"you at\",\n",
    "    re.compile(r\"\\byes'm\\b\", re.I | re.U): \"yes madam\",\n",
    "    re.compile(r\"\\byever\\b\", re.I | re.U): \"have you ever\",\n",
    "    re.compile(r\"\\by'know\\b\", re.I | re.U): \"you know\",\n",
    "    re.compile(r\"\\byessir\\b\", re.I | re.U): \"yes sir\",\n",
    "    re.compile(r\"\\byou're\\b\", re.I | re.U): \"you are\",\n",
    "    re.compile(r\"\\byou've\\b\", re.I | re.U): \"you have\",\n",
    "    re.compile(r\"\\bhow's\\b\", re.I | re.U): \"is\",\n",
    "    re.compile(r\"\\bI'd\\b\", re.I | re.U): \"I would\",\n",
    "    re.compile(r\"\\bI'll\\b\", re.I | re.U): \"I will\",\n",
    "    re.compile(r\"\\bit'll\\b\", re.I | re.U): \"it will\",\n",
    "    re.compile(r\"\\bit's\\b\", re.I | re.U): \"it is\",\n",
    "    re.compile(r\"\\bshe'd\\b\", re.I | re.U): \"she would\",\n",
    "    re.compile(r\"\\bshe'll\\b\", re.I | re.U): \"she will\",\n",
    "    re.compile(r\"\\bshe's\\b\", re.I | re.U): \"she is\",\n",
    "    re.compile(r\"\\bsomebody's\\b\", re.I | re.U): \"somebody is\",\n",
    "    re.compile(r\"\\bsomeone's\\b\", re.I | re.U): \"someone is\",\n",
    "    re.compile(r\"\\bsomething's\\b\", re.I | re.U): \"something is\",\n",
    "    re.compile(r\"\\bso's\\b\", re.I | re.U): \"so is\",\n",
    "    re.compile(r\"\\bthat'll\\b\", re.I | re.U): \"that will\",\n",
    "    re.compile(r\"\\bthat's\\b\", re.I | re.U): \"that is\",\n",
    "    re.compile(r\"\\bthat'd\\b\", re.I | re.U): \"that would\",\n",
    "    re.compile(r\"\\bthere'd\\b\", re.I | re.U): \"there would\",\n",
    "    re.compile(r\"\\bthere'll\\b\", re.I | re.U): \"there will\",\n",
    "    re.compile(r\"\\bthere's\\b\", re.I | re.U): \"there is\",\n",
    "    re.compile(r\"\\bthey'd\\b\", re.I | re.U): \"they would\",\n",
    "    re.compile(r\"\\bthey'd've\\b\", re.I | re.U): \"they would have\",\n",
    "    re.compile(r\"\\bthey'll\\b\", re.I | re.U): \"they will\",\n",
    "    re.compile(r\"\\bthey're\\b\", re.I | re.U): \"they are\",\n",
    "    re.compile(r\"\\bthis's\\b\", re.I | re.U): \"this is\",\n",
    "    re.compile(r\"\\bwe'd\\b\", re.I | re.U): \"we would\",\n",
    "    re.compile(r\"\\bwe'll\\b\", re.I | re.U): \"we will\",\n",
    "    re.compile(r\"\\bwhat'll\\b\", re.I | re.U): \"what will\",\n",
    "    re.compile(r\"\\bwhat're\\b\", re.I | re.U): \"what are\",\n",
    "    re.compile(r\"\\bwhat's\\b\", re.I | re.U): \"what is\",\n",
    "    re.compile(r\"\\bwhen's\\b\", re.I | re.U): \"when is\",\n",
    "    re.compile(r\"\\bwhere'll\\b\", re.I | re.U): \"where will\",\n",
    "    re.compile(r\"\\bwhere's\\b\", re.I | re.U): \"where is\",\n",
    "    re.compile(r\"\\bwhich'd\\b\", re.I | re.U): \"which would\",\n",
    "    re.compile(r\"\\bwhich'll\\b\", re.I | re.U): \"which will\",\n",
    "    re.compile(r\"\\bwhich's\\b\", re.I | re.U): \"which is\",\n",
    "    re.compile(r\"\\bwho'd\\b\", re.I | re.U): \"who would\",\n",
    "    re.compile(r\"\\bwho'll\\b\", re.I | re.U): \"who will\",\n",
    "    re.compile(r\"\\bwho's\\b\", re.I | re.U): \"who is\",\n",
    "    re.compile(r\"\\bwhy's\\b\", re.I | re.U):  \"why is'\",\n",
    "    re.compile(r\"\\by'ain't\\b\", re.I | re.U): \"you are not\",\n",
    "    re.compile(r\"\\byou'd\\b\", re.I | re.U): \"you would\",\n",
    "    re.compile(r\"\\byou'll\\b\", re.I | re.U): \"you will\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>star_rating</th>\n",
       "      <th>review_text</th>\n",
       "      <th>sentiment_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>and the shredder was dirty and the bin was par...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>one star worked about a month then died</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>one star the phone did not work no dial tone n...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>one star not laminated and no reinforced holes...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>one star cartridge was over filled black smear...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  star_rating                                        review_text  \\\n",
       "0           1  and the shredder was dirty and the bin was par...   \n",
       "1           1            one star worked about a month then died   \n",
       "2           1  one star the phone did not work no dial tone n...   \n",
       "3           1  one star not laminated and no reinforced holes...   \n",
       "4           1  one star cartridge was over filled black smear...   \n",
       "\n",
       "   sentiment_score  \n",
       "0                1  \n",
       "1                1  \n",
       "2                1  \n",
       "3                1  \n",
       "4                1  "
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# expand any contractions present in text\n",
    "def expand_contractions(text):\n",
    "    for pattern, substitute in contractions.items():\n",
    "        updated_text = pattern.sub(substitute, text)\n",
    "\n",
    "    return updated_text\n",
    "\n",
    "\n",
    "df[\"review_text\"] = df[\"review_text\"].apply(expand_contractions)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>star_rating</th>\n",
       "      <th>review_text</th>\n",
       "      <th>sentiment_score</th>\n",
       "      <th>tokenised_review_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>and the shredder was dirty and the bin was par...</td>\n",
       "      <td>1</td>\n",
       "      <td>[and, the, shredder, was, dirty, and, the, bin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>one star worked about a month then died</td>\n",
       "      <td>1</td>\n",
       "      <td>[one, star, worked, about, a, month, then, died]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>one star the phone did not work no dial tone n...</td>\n",
       "      <td>1</td>\n",
       "      <td>[one, star, the, phone, did, not, work, no, di...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>one star not laminated and no reinforced holes...</td>\n",
       "      <td>1</td>\n",
       "      <td>[one, star, not, laminated, and, no, reinforce...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>one star cartridge was over filled black smear...</td>\n",
       "      <td>1</td>\n",
       "      <td>[one, star, cartridge, was, over, filled, blac...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  star_rating                                        review_text  \\\n",
       "0           1  and the shredder was dirty and the bin was par...   \n",
       "1           1            one star worked about a month then died   \n",
       "2           1  one star the phone did not work no dial tone n...   \n",
       "3           1  one star not laminated and no reinforced holes...   \n",
       "4           1  one star cartridge was over filled black smear...   \n",
       "\n",
       "   sentiment_score                              tokenised_review_text  \n",
       "0                1  [and, the, shredder, was, dirty, and, the, bin...  \n",
       "1                1   [one, star, worked, about, a, month, then, died]  \n",
       "2                1  [one, star, the, phone, did, not, work, no, di...  \n",
       "3                1  [one, star, not, laminated, and, no, reinforce...  \n",
       "4                1  [one, star, cartridge, was, over, filled, blac...  "
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "df[\"tokenised_review_text\"] = df[\"review_text\"].apply(word_tokenize)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Google W2V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "\n",
    "google_w2v: Word2Vec | KeyedVectors = api.load(\"word2vec-google-news-300\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('glad', 0.6133241057395935), ('thrilled', 0.5700709223747253), ('excited', 0.537404477596283), ('delighted', 0.5340354442596436), ('pleased', 0.5313177108764648)]\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    google_w2v.most_similar(\n",
    "        positive=[\"nice\", \"happy\"],\n",
    "        negative=[\"bad\"], topn=5\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.52264345\n"
     ]
    }
   ],
   "source": [
    "print(google_w2v.similarity(\"best\", \"better\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Custom W2V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_w2v = Word2Vec(\n",
    "    sentences=df[\"tokenised_review_text\"],\n",
    "    workers=1,\n",
    "    vector_size=300,\n",
    "    window=11\n",
    ")\n",
    "custom_w2v = custom_w2v.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('pleased', 0.580795407295227), ('satisfied', 0.5139424800872803), ('thrilled', 0.47956201434135437), ('disapointed', 0.45218977332115173), ('lovely', 0.4362533390522003)]\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    custom_w2v.most_similar(\n",
    "        positive=[\"nice\", \"happy\"],\n",
    "        negative=[\"bad\"], topn=5\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3370654\n"
     ]
    }
   ],
   "source": [
    "print(custom_w2v.similarity(\"best\", \"better\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMeanW2VFeatures(tokens, w2v_model: Word2Vec | KeyedVectors):\n",
    "    if (len(tokens) == 0):\n",
    "        v = torch.zeros(300)\n",
    "        v.to(torch.float32)\n",
    "        return v\n",
    "\n",
    "    return w2v_model.get_mean_vector(tokens, ignore_missing=True)\n",
    "\n",
    "\n",
    "def getConcatW2VFeatures(tokens, w2v_model: Word2Vec | KeyedVectors):\n",
    "    series = []\n",
    "    for i in tokens:\n",
    "        if i in w2v_model:\n",
    "            series.append(w2v_model[i])\n",
    "\n",
    "        if len(series) == 10:\n",
    "            break\n",
    "\n",
    "    if len(series) != 10:\n",
    "        remaining_len = 10 - len(series)\n",
    "        for i in range(remaining_len):\n",
    "            v = torch.zeros(300)\n",
    "            v.to(torch.float32)\n",
    "            series.append(v)\n",
    "\n",
    "    series = np.array([j for i in series for j in i])\n",
    "\n",
    "    return series\n",
    "\n",
    "\n",
    "def getFeaturesDataFrame(series: pd.Series):\n",
    "    return pd.DataFrame.from_records(series.values)\n",
    "\n",
    "\n",
    "def filterNeutralReviewsMeanFeatures(df: pd.DataFrame, w2v_model: Word2Vec | KeyedVectors):\n",
    "    df = df.loc[df[\"sentiment_score\"] != 3]\n",
    "    X = df[\"tokenised_review_text\"].apply(\n",
    "        lambda x: getMeanW2VFeatures(x, w2v_model)\n",
    "    )\n",
    "    Y = df[\"sentiment_score\"]\n",
    "    return X, Y\n",
    "\n",
    "\n",
    "def filterNeutralReviewsConcatFeatures(df: pd.DataFrame, w2v_model: Word2Vec | KeyedVectors):\n",
    "    df = df.loc[df[\"sentiment_score\"] != 3]\n",
    "    X = df[\"tokenised_review_text\"].apply(\n",
    "        lambda x: getConcatW2VFeatures(x, w2v_model)\n",
    "    )\n",
    "\n",
    "    Y = df[\"sentiment_score\"]\n",
    "    return X, Y\n",
    "\n",
    "\n",
    "def getAllReviewsMeanFeatures(df: pd.DataFrame, w2v_model: Word2Vec | KeyedVectors):\n",
    "    X = df[\"tokenised_review_text\"].apply(\n",
    "        lambda x: getMeanW2VFeatures(x, w2v_model)\n",
    "    )\n",
    "\n",
    "    Y = df[\"sentiment_score\"]\n",
    "    return X, Y\n",
    "\n",
    "\n",
    "def getAllReviewsConcatFeatures(df: pd.DataFrame, w2v_model: Word2Vec | KeyedVectors):\n",
    "    X = df[\"tokenised_review_text\"].apply(\n",
    "        lambda x: getConcatW2VFeatures(x, w2v_model)\n",
    "    )\n",
    "\n",
    "    Y = df[\"sentiment_score\"]\n",
    "    return X, Y\n",
    "\n",
    "\n",
    "def getTrainTestSplit(X, Y):\n",
    "    return train_test_split(X, Y, test_size=0.2, random_state=10)\n",
    "\n",
    "\n",
    "def get_accuracy(actual, predicted):\n",
    "    report = classification_report(actual, predicted, output_dict=True)\n",
    "    accuracy = report[\"accuracy\"]\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_columns = [\n",
    "    \"model\",\n",
    "    \"classification_type\",\n",
    "    \"vector_type\",\n",
    "    \"w2v_model_type\",\n",
    "    \"accuracy\"\n",
    "]\n",
    "final_report = pd.DataFrame(columns=report_columns)\n",
    "\n",
    "\n",
    "def add_report_entry(model, classification_type, vector_type, w2v_model_type, accuracy):\n",
    "    global final_report\n",
    "    data = {\n",
    "        \"model\": model,\n",
    "        \"classification_type\": classification_type,\n",
    "        \"vector_type\": vector_type,\n",
    "        \"w2v_model_type\": w2v_model_type,\n",
    "        \"accuracy\": accuracy\n",
    "    }\n",
    "\n",
    "    final_report.loc[len(final_report)] = data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Binary Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perceptron - Pretrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perceptron Scores: \n",
      "Pretained model: \n",
      "Accuracy:  0.7333\n"
     ]
    }
   ],
   "source": [
    "X, Y = filterNeutralReviewsMeanFeatures(df, google_w2v)\n",
    "X = getFeaturesDataFrame(X)\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = getTrainTestSplit(X, Y)\n",
    "\n",
    "classifier = Perceptron(random_state=5)\n",
    "\n",
    "classifier.fit(X_train, Y_train)\n",
    "\n",
    "Y_pred = classifier.predict(X_test)\n",
    "\n",
    "accuracy = get_accuracy(Y_test, Y_pred)\n",
    "\n",
    "add_report_entry(\"Perceptron\", \"Binary\", \"Mean\", \"Pretrained\", accuracy)\n",
    "\n",
    "print(\"Perceptron Scores: \")\n",
    "print(\"Pretained model: \")\n",
    "print(\"Accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perceptron - Custom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perceptron Scores: \n",
      "Pretained model: \n",
      "Accuracy:  0.8861\n"
     ]
    }
   ],
   "source": [
    "X, Y = filterNeutralReviewsMeanFeatures(df, custom_w2v)\n",
    "X = getFeaturesDataFrame(X)\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = getTrainTestSplit(X, Y)\n",
    "\n",
    "classifier = Perceptron(random_state=5)\n",
    "\n",
    "classifier.fit(X_train, Y_train)\n",
    "\n",
    "Y_pred = classifier.predict(X_test)\n",
    "\n",
    "accuracy = get_accuracy(Y_test, Y_pred)\n",
    "\n",
    "add_report_entry(\"Perceptron\", \"Binary\", \"Mean\", \"Custom\", accuracy)\n",
    "\n",
    "print(\"Perceptron Scores: \")\n",
    "print(\"Pretained model: \")\n",
    "print(\"Accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM - Pretrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Scores: \n",
      "Pretained model: \n",
      "Accuracy:  0.88445\n"
     ]
    }
   ],
   "source": [
    "X, Y = filterNeutralReviewsMeanFeatures(df, google_w2v)\n",
    "X = getFeaturesDataFrame(X)\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = getTrainTestSplit(X, Y)\n",
    "\n",
    "classifier = LinearSVC(random_state=5, dual=True)\n",
    "\n",
    "classifier.fit(X_train, Y_train)\n",
    "\n",
    "Y_pred = classifier.predict(X_test)\n",
    "\n",
    "accuracy = get_accuracy(Y_test, Y_pred)\n",
    "\n",
    "add_report_entry(\"SVM\", \"Binary\", \"Mean\", \"Pretrained\", accuracy)\n",
    "\n",
    "print(\"SVM Scores: \")\n",
    "print(\"Pretained model: \")\n",
    "print(\"Accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM - Custom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Scores: \n",
      "Custom model: \n",
      "Accuracy:  0.910525\n"
     ]
    }
   ],
   "source": [
    "X, Y = filterNeutralReviewsMeanFeatures(df, custom_w2v)\n",
    "X = getFeaturesDataFrame(X)\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = getTrainTestSplit(X, Y)\n",
    "\n",
    "classifier = LinearSVC(random_state=5, dual=True)\n",
    "\n",
    "classifier.fit(X_train, Y_train)\n",
    "\n",
    "Y_pred = classifier.predict(X_test)\n",
    "\n",
    "accuracy = get_accuracy(Y_test, Y_pred)\n",
    "\n",
    "add_report_entry(\"SVM\", \"Binary\", \"Mean\", \"Custom\", accuracy)\n",
    "\n",
    "print(\"SVM Scores: \")\n",
    "print(\"Custom model: \")\n",
    "print(\"Accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, X, Y):\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.X[index], self.Y[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDatasetAndDataLoader(X, Y):\n",
    "    X_train, X_test, Y_train, Y_test = getTrainTestSplit(X, Y)\n",
    "\n",
    "    train_dataset = CustomDataset(X_train.reset_index(drop=True), Y_train.reset_index(drop=True))\n",
    "    test_dataset = CustomDataset(X_test.reset_index(drop=True), Y_test.reset_index(drop=True))\n",
    "    \n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=False)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "    return train_dataloader, test_dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FFNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class FFNN(nn.Module):\n",
    "    def __init__(self, input_features, output_features):\n",
    "        super(FFNN, self).__init__()\n",
    "        hidden_layer1_features = 50\n",
    "        hidden_layer2_features = 10\n",
    "        output_layer_features = output_features * 2\n",
    "\n",
    "        self.input = nn.Linear(\n",
    "            in_features=input_features,\n",
    "            out_features=hidden_layer1_features\n",
    "        )\n",
    "\n",
    "        self.hidden1 = nn.Linear(\n",
    "            in_features=hidden_layer1_features,\n",
    "            out_features=hidden_layer2_features\n",
    "        )\n",
    "\n",
    "        self.hidden2 = nn.Linear(\n",
    "            in_features=hidden_layer2_features,\n",
    "            out_features=output_layer_features\n",
    "        )\n",
    "\n",
    "        self.output = nn.Linear(\n",
    "            in_features=output_layer_features,\n",
    "            out_features=output_features\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.input(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        x = self.hidden1(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        x = self.hidden2(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        x = self.output(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ffnn_model(dataloader, classes, input_feature_len=300):\n",
    "    model = FFNN(input_feature_len, classes)\n",
    "\n",
    "    learning_rate = 0.01\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "    no_epochs = 100\n",
    "\n",
    "    for epoch in range(no_epochs):\n",
    "        train_loss = 0.0\n",
    "\n",
    "        model.train()\n",
    "        \n",
    "        for data, target in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            output = model(data)\n",
    "\n",
    "            loss = criterion(output, target - 1)\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item() * data.size(0)\n",
    "\n",
    "        train_loss = train_loss/len(dataloader.dataset)\n",
    "\n",
    "        print(\n",
    "            'Epoch: {} \\tTraining Loss: {:.6f}'.format(\n",
    "                epoch+1,\n",
    "                train_loss,\n",
    "            )\n",
    "        )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, dataloader):\n",
    "    y_actual = []\n",
    "    y_pred = []\n",
    "    for data, actual in dataloader:\n",
    "        outputs = model(data) \n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        y_pred.append(predicted.cpu().item() + 1)\n",
    "        y_actual.append(actual) \n",
    "\n",
    "    return np.array(y_actual), np.array(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Binary Classification with mean vector (Pretrained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.692940\n",
      "Epoch: 2 \tTraining Loss: 0.691403\n",
      "Epoch: 3 \tTraining Loss: 0.686447\n",
      "Epoch: 4 \tTraining Loss: 0.629459\n",
      "Epoch: 5 \tTraining Loss: 0.446910\n",
      "Epoch: 6 \tTraining Loss: 0.358433\n",
      "Epoch: 7 \tTraining Loss: 0.331845\n",
      "Epoch: 8 \tTraining Loss: 0.320533\n",
      "Epoch: 9 \tTraining Loss: 0.313366\n",
      "Epoch: 10 \tTraining Loss: 0.307571\n",
      "Epoch: 11 \tTraining Loss: 0.302515\n",
      "Epoch: 12 \tTraining Loss: 0.298090\n",
      "Epoch: 13 \tTraining Loss: 0.293761\n",
      "Epoch: 14 \tTraining Loss: 0.289751\n",
      "Epoch: 15 \tTraining Loss: 0.285840\n",
      "Epoch: 16 \tTraining Loss: 0.281922\n",
      "Epoch: 17 \tTraining Loss: 0.278182\n",
      "Epoch: 18 \tTraining Loss: 0.274485\n",
      "Epoch: 19 \tTraining Loss: 0.270882\n",
      "Epoch: 20 \tTraining Loss: 0.267308\n",
      "Epoch: 21 \tTraining Loss: 0.264146\n",
      "Epoch: 22 \tTraining Loss: 0.261026\n",
      "Epoch: 23 \tTraining Loss: 0.258227\n",
      "Epoch: 24 \tTraining Loss: 0.255778\n",
      "Epoch: 25 \tTraining Loss: 0.253437\n",
      "Epoch: 26 \tTraining Loss: 0.251016\n",
      "Epoch: 27 \tTraining Loss: 0.249163\n",
      "Epoch: 28 \tTraining Loss: 0.247307\n",
      "Epoch: 29 \tTraining Loss: 0.245516\n",
      "Epoch: 30 \tTraining Loss: 0.243794\n",
      "Epoch: 31 \tTraining Loss: 0.242498\n",
      "Epoch: 32 \tTraining Loss: 0.241089\n",
      "Epoch: 33 \tTraining Loss: 0.239668\n",
      "Epoch: 34 \tTraining Loss: 0.238497\n",
      "Epoch: 35 \tTraining Loss: 0.237365\n",
      "Epoch: 36 \tTraining Loss: 0.236282\n",
      "Epoch: 37 \tTraining Loss: 0.235249\n",
      "Epoch: 38 \tTraining Loss: 0.234233\n",
      "Epoch: 39 \tTraining Loss: 0.233229\n",
      "Epoch: 40 \tTraining Loss: 0.232368\n",
      "Epoch: 41 \tTraining Loss: 0.231476\n",
      "Epoch: 42 \tTraining Loss: 0.230620\n",
      "Epoch: 43 \tTraining Loss: 0.229776\n",
      "Epoch: 44 \tTraining Loss: 0.228954\n",
      "Epoch: 45 \tTraining Loss: 0.228175\n",
      "Epoch: 46 \tTraining Loss: 0.227549\n",
      "Epoch: 47 \tTraining Loss: 0.226801\n",
      "Epoch: 48 \tTraining Loss: 0.226164\n",
      "Epoch: 49 \tTraining Loss: 0.225527\n",
      "Epoch: 50 \tTraining Loss: 0.225010\n",
      "Epoch: 51 \tTraining Loss: 0.224302\n",
      "Epoch: 52 \tTraining Loss: 0.223745\n",
      "Epoch: 53 \tTraining Loss: 0.223184\n",
      "Epoch: 54 \tTraining Loss: 0.222509\n",
      "Epoch: 55 \tTraining Loss: 0.221963\n",
      "Epoch: 56 \tTraining Loss: 0.221504\n",
      "Epoch: 57 \tTraining Loss: 0.220928\n",
      "Epoch: 58 \tTraining Loss: 0.220410\n",
      "Epoch: 59 \tTraining Loss: 0.219928\n",
      "Epoch: 60 \tTraining Loss: 0.219385\n",
      "Epoch: 61 \tTraining Loss: 0.218895\n",
      "Epoch: 62 \tTraining Loss: 0.218548\n",
      "Epoch: 63 \tTraining Loss: 0.218136\n",
      "Epoch: 64 \tTraining Loss: 0.217769\n",
      "Epoch: 65 \tTraining Loss: 0.217298\n",
      "Epoch: 66 \tTraining Loss: 0.216753\n",
      "Epoch: 67 \tTraining Loss: 0.216494\n",
      "Epoch: 68 \tTraining Loss: 0.215994\n",
      "Epoch: 69 \tTraining Loss: 0.215666\n",
      "Epoch: 70 \tTraining Loss: 0.215238\n",
      "Epoch: 71 \tTraining Loss: 0.214879\n",
      "Epoch: 72 \tTraining Loss: 0.214434\n",
      "Epoch: 73 \tTraining Loss: 0.214133\n",
      "Epoch: 74 \tTraining Loss: 0.213806\n",
      "Epoch: 75 \tTraining Loss: 0.213481\n",
      "Epoch: 76 \tTraining Loss: 0.213109\n",
      "Epoch: 77 \tTraining Loss: 0.212830\n",
      "Epoch: 78 \tTraining Loss: 0.212410\n",
      "Epoch: 79 \tTraining Loss: 0.212170\n",
      "Epoch: 80 \tTraining Loss: 0.211741\n",
      "Epoch: 81 \tTraining Loss: 0.211458\n",
      "Epoch: 82 \tTraining Loss: 0.211076\n",
      "Epoch: 83 \tTraining Loss: 0.210747\n",
      "Epoch: 84 \tTraining Loss: 0.210547\n",
      "Epoch: 85 \tTraining Loss: 0.210145\n",
      "Epoch: 86 \tTraining Loss: 0.209732\n",
      "Epoch: 87 \tTraining Loss: 0.209462\n",
      "Epoch: 88 \tTraining Loss: 0.209172\n",
      "Epoch: 89 \tTraining Loss: 0.208877\n",
      "Epoch: 90 \tTraining Loss: 0.208588\n",
      "Epoch: 91 \tTraining Loss: 0.208302\n",
      "Epoch: 92 \tTraining Loss: 0.208014\n",
      "Epoch: 93 \tTraining Loss: 0.207770\n",
      "Epoch: 94 \tTraining Loss: 0.207543\n",
      "Epoch: 95 \tTraining Loss: 0.207261\n",
      "Epoch: 96 \tTraining Loss: 0.206972\n",
      "Epoch: 97 \tTraining Loss: 0.206686\n",
      "Epoch: 98 \tTraining Loss: 0.206432\n",
      "Epoch: 99 \tTraining Loss: 0.206121\n",
      "Epoch: 100 \tTraining Loss: 0.205832\n",
      "FFNN Scores: \n",
      "Google model: \n",
      "Average Vector: \n",
      "Accuracy:  0.911825\n"
     ]
    }
   ],
   "source": [
    "X, Y = filterNeutralReviewsMeanFeatures(df, google_w2v)\n",
    "\n",
    "train, test = getDatasetAndDataLoader(X, Y)\n",
    "\n",
    "classifier = train_ffnn_model(train, 2)\n",
    "\n",
    "actual, prediction = predict(classifier, test)\n",
    "\n",
    "accuracy = get_accuracy(actual, prediction)\n",
    "\n",
    "add_report_entry(\"FFNN\", \"Binary\", \"Mean\", \"Pretrained\", accuracy)\n",
    "\n",
    "print(\"FFNN Scores: \")\n",
    "print(\"Google model: \")\n",
    "print(\"Average Vector: \")\n",
    "print(\"Accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ternary Classification with mean vector (Pretrained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 1.056475\n",
      "Epoch: 2 \tTraining Loss: 1.054947\n",
      "Epoch: 3 \tTraining Loss: 1.054061\n",
      "Epoch: 4 \tTraining Loss: 1.050595\n",
      "Epoch: 5 \tTraining Loss: 1.015088\n",
      "Epoch: 6 \tTraining Loss: 0.837232\n",
      "Epoch: 7 \tTraining Loss: 0.746839\n",
      "Epoch: 8 \tTraining Loss: 0.726208\n",
      "Epoch: 9 \tTraining Loss: 0.715880\n",
      "Epoch: 10 \tTraining Loss: 0.707310\n",
      "Epoch: 11 \tTraining Loss: 0.699763\n",
      "Epoch: 12 \tTraining Loss: 0.691826\n",
      "Epoch: 13 \tTraining Loss: 0.685205\n",
      "Epoch: 14 \tTraining Loss: 0.678978\n",
      "Epoch: 15 \tTraining Loss: 0.672775\n",
      "Epoch: 16 \tTraining Loss: 0.665975\n",
      "Epoch: 17 \tTraining Loss: 0.658729\n",
      "Epoch: 18 \tTraining Loss: 0.650618\n",
      "Epoch: 19 \tTraining Loss: 0.640998\n",
      "Epoch: 20 \tTraining Loss: 0.628896\n",
      "Epoch: 21 \tTraining Loss: 0.616085\n",
      "Epoch: 22 \tTraining Loss: 0.604748\n",
      "Epoch: 23 \tTraining Loss: 0.595516\n",
      "Epoch: 24 \tTraining Loss: 0.587288\n",
      "Epoch: 25 \tTraining Loss: 0.580301\n",
      "Epoch: 26 \tTraining Loss: 0.574120\n",
      "Epoch: 27 \tTraining Loss: 0.568939\n",
      "Epoch: 28 \tTraining Loss: 0.564610\n",
      "Epoch: 29 \tTraining Loss: 0.560863\n",
      "Epoch: 30 \tTraining Loss: 0.557645\n",
      "Epoch: 31 \tTraining Loss: 0.554723\n",
      "Epoch: 32 \tTraining Loss: 0.552053\n",
      "Epoch: 33 \tTraining Loss: 0.549620\n",
      "Epoch: 34 \tTraining Loss: 0.547428\n",
      "Epoch: 35 \tTraining Loss: 0.545438\n",
      "Epoch: 36 \tTraining Loss: 0.543564\n",
      "Epoch: 37 \tTraining Loss: 0.541884\n",
      "Epoch: 38 \tTraining Loss: 0.540333\n",
      "Epoch: 39 \tTraining Loss: 0.538862\n",
      "Epoch: 40 \tTraining Loss: 0.537299\n",
      "Epoch: 41 \tTraining Loss: 0.535942\n",
      "Epoch: 42 \tTraining Loss: 0.534512\n",
      "Epoch: 43 \tTraining Loss: 0.533296\n",
      "Epoch: 44 \tTraining Loss: 0.532035\n",
      "Epoch: 45 \tTraining Loss: 0.530848\n",
      "Epoch: 46 \tTraining Loss: 0.529760\n",
      "Epoch: 47 \tTraining Loss: 0.528685\n",
      "Epoch: 48 \tTraining Loss: 0.527563\n",
      "Epoch: 49 \tTraining Loss: 0.526540\n",
      "Epoch: 50 \tTraining Loss: 0.525523\n",
      "Epoch: 51 \tTraining Loss: 0.524506\n",
      "Epoch: 52 \tTraining Loss: 0.523587\n",
      "Epoch: 53 \tTraining Loss: 0.522592\n",
      "Epoch: 54 \tTraining Loss: 0.521785\n",
      "Epoch: 55 \tTraining Loss: 0.520993\n",
      "Epoch: 56 \tTraining Loss: 0.520086\n",
      "Epoch: 57 \tTraining Loss: 0.519249\n",
      "Epoch: 58 \tTraining Loss: 0.518452\n",
      "Epoch: 59 \tTraining Loss: 0.517661\n",
      "Epoch: 60 \tTraining Loss: 0.516872\n",
      "Epoch: 61 \tTraining Loss: 0.516046\n",
      "Epoch: 62 \tTraining Loss: 0.515351\n",
      "Epoch: 63 \tTraining Loss: 0.514580\n",
      "Epoch: 64 \tTraining Loss: 0.513859\n",
      "Epoch: 65 \tTraining Loss: 0.513105\n",
      "Epoch: 66 \tTraining Loss: 0.512395\n",
      "Epoch: 67 \tTraining Loss: 0.511776\n",
      "Epoch: 68 \tTraining Loss: 0.511069\n",
      "Epoch: 69 \tTraining Loss: 0.510371\n",
      "Epoch: 70 \tTraining Loss: 0.509701\n",
      "Epoch: 71 \tTraining Loss: 0.509042\n",
      "Epoch: 72 \tTraining Loss: 0.508363\n",
      "Epoch: 73 \tTraining Loss: 0.507600\n",
      "Epoch: 74 \tTraining Loss: 0.507006\n",
      "Epoch: 75 \tTraining Loss: 0.506432\n",
      "Epoch: 76 \tTraining Loss: 0.505795\n",
      "Epoch: 77 \tTraining Loss: 0.505136\n",
      "Epoch: 78 \tTraining Loss: 0.504601\n",
      "Epoch: 79 \tTraining Loss: 0.503954\n",
      "Epoch: 80 \tTraining Loss: 0.503338\n",
      "Epoch: 81 \tTraining Loss: 0.502765\n",
      "Epoch: 82 \tTraining Loss: 0.502087\n",
      "Epoch: 83 \tTraining Loss: 0.501574\n",
      "Epoch: 84 \tTraining Loss: 0.501006\n",
      "Epoch: 85 \tTraining Loss: 0.500467\n",
      "Epoch: 86 \tTraining Loss: 0.499987\n",
      "Epoch: 87 \tTraining Loss: 0.499494\n",
      "Epoch: 88 \tTraining Loss: 0.498915\n",
      "Epoch: 89 \tTraining Loss: 0.498401\n",
      "Epoch: 90 \tTraining Loss: 0.498004\n",
      "Epoch: 91 \tTraining Loss: 0.497532\n",
      "Epoch: 92 \tTraining Loss: 0.496985\n",
      "Epoch: 93 \tTraining Loss: 0.496515\n",
      "Epoch: 94 \tTraining Loss: 0.496075\n",
      "Epoch: 95 \tTraining Loss: 0.495688\n",
      "Epoch: 96 \tTraining Loss: 0.495221\n",
      "Epoch: 97 \tTraining Loss: 0.494822\n",
      "Epoch: 98 \tTraining Loss: 0.494376\n",
      "Epoch: 99 \tTraining Loss: 0.493939\n",
      "Epoch: 100 \tTraining Loss: 0.493561\n",
      "FFNN Scores: \n",
      "Google model: \n",
      "Average Vector: \n",
      "Accuracy:  0.7852\n"
     ]
    }
   ],
   "source": [
    "X, Y = getAllReviewsMeanFeatures(df, google_w2v)\n",
    "\n",
    "train, test = getDatasetAndDataLoader(X, Y)\n",
    "\n",
    "classifier = train_ffnn_model(train, 3)\n",
    "\n",
    "actual, prediction = predict(classifier, test)\n",
    "\n",
    "accuracy = get_accuracy(actual, prediction)\n",
    "\n",
    "add_report_entry(\"FFNN\", \"Ternary\", \"Mean\", \"Pretrained\", accuracy)\n",
    "\n",
    "print(\"FFNN Scores: \")\n",
    "print(\"Google model: \")\n",
    "print(\"Average Vector: \")\n",
    "print(\"Accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Binary Classification with Concat Vector (Pretrained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.675285\n",
      "Epoch: 2 \tTraining Loss: 0.335751\n",
      "Epoch: 3 \tTraining Loss: 0.263348\n",
      "Epoch: 4 \tTraining Loss: 0.248903\n",
      "Epoch: 5 \tTraining Loss: 0.237827\n",
      "Epoch: 6 \tTraining Loss: 0.227216\n",
      "Epoch: 7 \tTraining Loss: 0.216635\n",
      "Epoch: 8 \tTraining Loss: 0.205671\n",
      "Epoch: 9 \tTraining Loss: 0.193936\n",
      "Epoch: 10 \tTraining Loss: 0.181323\n",
      "Epoch: 11 \tTraining Loss: 0.167864\n",
      "Epoch: 12 \tTraining Loss: 0.153571\n",
      "Epoch: 13 \tTraining Loss: 0.138774\n",
      "Epoch: 14 \tTraining Loss: 0.123703\n",
      "Epoch: 15 \tTraining Loss: 0.108493\n",
      "Epoch: 16 \tTraining Loss: 0.093050\n",
      "Epoch: 17 \tTraining Loss: 0.079160\n",
      "Epoch: 18 \tTraining Loss: 0.065893\n",
      "Epoch: 19 \tTraining Loss: 0.059706\n",
      "Epoch: 20 \tTraining Loss: 0.057576\n",
      "Epoch: 21 \tTraining Loss: 0.053008\n",
      "Epoch: 22 \tTraining Loss: 0.048771\n",
      "Epoch: 23 \tTraining Loss: 0.041851\n",
      "Epoch: 24 \tTraining Loss: 0.036055\n",
      "Epoch: 25 \tTraining Loss: 0.031889\n",
      "Epoch: 26 \tTraining Loss: 0.029433\n",
      "Epoch: 27 \tTraining Loss: 0.027157\n",
      "Epoch: 28 \tTraining Loss: 0.022913\n",
      "Epoch: 29 \tTraining Loss: 0.019890\n",
      "Epoch: 30 \tTraining Loss: 0.019499\n",
      "Epoch: 31 \tTraining Loss: 0.018846\n",
      "Epoch: 32 \tTraining Loss: 0.016817\n",
      "Epoch: 33 \tTraining Loss: 0.013260\n",
      "Epoch: 34 \tTraining Loss: 0.011647\n",
      "Epoch: 35 \tTraining Loss: 0.011345\n",
      "Epoch: 36 \tTraining Loss: 0.011753\n",
      "Epoch: 37 \tTraining Loss: 0.009082\n",
      "Epoch: 38 \tTraining Loss: 0.007318\n",
      "Epoch: 39 \tTraining Loss: 0.006632\n",
      "Epoch: 40 \tTraining Loss: 0.005380\n",
      "Epoch: 41 \tTraining Loss: 0.004579\n",
      "Epoch: 42 \tTraining Loss: 0.004095\n",
      "Epoch: 43 \tTraining Loss: 0.003540\n",
      "Epoch: 44 \tTraining Loss: 0.003249\n",
      "Epoch: 45 \tTraining Loss: 0.002948\n",
      "Epoch: 46 \tTraining Loss: 0.002609\n",
      "Epoch: 47 \tTraining Loss: 0.002342\n",
      "Epoch: 48 \tTraining Loss: 0.002082\n",
      "Epoch: 49 \tTraining Loss: 0.002047\n",
      "Epoch: 50 \tTraining Loss: 0.001777\n",
      "Epoch: 51 \tTraining Loss: 0.001596\n",
      "Epoch: 52 \tTraining Loss: 0.001805\n",
      "Epoch: 53 \tTraining Loss: 0.001518\n",
      "Epoch: 54 \tTraining Loss: 0.001520\n",
      "Epoch: 55 \tTraining Loss: 0.001294\n",
      "Epoch: 56 \tTraining Loss: 0.001310\n",
      "Epoch: 57 \tTraining Loss: 0.001184\n",
      "Epoch: 58 \tTraining Loss: 0.001175\n",
      "Epoch: 59 \tTraining Loss: 0.001084\n",
      "Epoch: 60 \tTraining Loss: 0.001042\n",
      "Epoch: 61 \tTraining Loss: 0.001023\n",
      "Epoch: 62 \tTraining Loss: 0.001043\n",
      "Epoch: 63 \tTraining Loss: 0.000964\n",
      "Epoch: 64 \tTraining Loss: 0.000943\n",
      "Epoch: 65 \tTraining Loss: 0.000908\n",
      "Epoch: 66 \tTraining Loss: 0.000947\n",
      "Epoch: 67 \tTraining Loss: 0.000905\n",
      "Epoch: 68 \tTraining Loss: 0.000878\n",
      "Epoch: 69 \tTraining Loss: 0.000852\n",
      "Epoch: 70 \tTraining Loss: 0.000833\n",
      "Epoch: 71 \tTraining Loss: 0.000801\n",
      "Epoch: 72 \tTraining Loss: 0.000759\n",
      "Epoch: 73 \tTraining Loss: 0.000715\n",
      "Epoch: 74 \tTraining Loss: 0.000676\n",
      "Epoch: 75 \tTraining Loss: 0.000675\n",
      "Epoch: 76 \tTraining Loss: 0.000635\n",
      "Epoch: 77 \tTraining Loss: 0.000651\n",
      "Epoch: 78 \tTraining Loss: 0.000615\n",
      "Epoch: 79 \tTraining Loss: 0.000626\n",
      "Epoch: 80 \tTraining Loss: 0.000597\n",
      "Epoch: 81 \tTraining Loss: 0.000606\n",
      "Epoch: 82 \tTraining Loss: 0.000562\n",
      "Epoch: 83 \tTraining Loss: 0.000542\n",
      "Epoch: 84 \tTraining Loss: 0.000555\n",
      "Epoch: 85 \tTraining Loss: 0.000520\n",
      "Epoch: 86 \tTraining Loss: 0.000535\n",
      "Epoch: 87 \tTraining Loss: 0.000507\n",
      "Epoch: 88 \tTraining Loss: 0.000502\n",
      "Epoch: 89 \tTraining Loss: 0.000523\n",
      "Epoch: 90 \tTraining Loss: 0.000494\n",
      "Epoch: 91 \tTraining Loss: 0.000489\n",
      "Epoch: 92 \tTraining Loss: 0.000509\n",
      "Epoch: 93 \tTraining Loss: 0.000483\n",
      "Epoch: 94 \tTraining Loss: 0.000477\n",
      "Epoch: 95 \tTraining Loss: 0.000476\n",
      "Epoch: 96 \tTraining Loss: 0.000498\n",
      "Epoch: 97 \tTraining Loss: 0.000465\n",
      "Epoch: 98 \tTraining Loss: 0.000448\n",
      "Epoch: 99 \tTraining Loss: 0.000469\n",
      "Epoch: 100 \tTraining Loss: 0.000437\n",
      "FFNN Scores: \n",
      "Google model: \n",
      "Average Vector: \n",
      "Accuracy:  0.88775\n"
     ]
    }
   ],
   "source": [
    "X, Y = filterNeutralReviewsConcatFeatures(df, google_w2v)\n",
    "\n",
    "train, test = getDatasetAndDataLoader(X, Y)\n",
    "\n",
    "classifier = train_ffnn_model(train, 2, 3000)\n",
    "\n",
    "actual, prediction = predict(classifier, test)\n",
    "\n",
    "accuracy = get_accuracy(actual, prediction)\n",
    "\n",
    "add_report_entry(\"FFNN\", \"Binary\", \"Concat\", \"Pretrained\", accuracy)\n",
    "\n",
    "print(\"FFNN Scores: \")\n",
    "print(\"Google model: \")\n",
    "print(\"Average Vector: \")\n",
    "print(\"Accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ternary Classification with Concat Vector (Pretrained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 1.057946\n",
      "Epoch: 2 \tTraining Loss: 0.747765\n",
      "Epoch: 3 \tTraining Loss: 0.597455\n",
      "Epoch: 4 \tTraining Loss: 0.530667\n",
      "Epoch: 5 \tTraining Loss: 0.497497\n",
      "Epoch: 6 \tTraining Loss: 0.476916\n",
      "Epoch: 7 \tTraining Loss: 0.460385\n",
      "Epoch: 8 \tTraining Loss: 0.445046\n",
      "Epoch: 9 \tTraining Loss: 0.430345\n",
      "Epoch: 10 \tTraining Loss: 0.415754\n",
      "Epoch: 11 \tTraining Loss: 0.401227\n",
      "Epoch: 12 \tTraining Loss: 0.387014\n",
      "Epoch: 13 \tTraining Loss: 0.372539\n",
      "Epoch: 14 \tTraining Loss: 0.358643\n",
      "Epoch: 15 \tTraining Loss: 0.344914\n",
      "Epoch: 16 \tTraining Loss: 0.331007\n",
      "Epoch: 17 \tTraining Loss: 0.318146\n",
      "Epoch: 18 \tTraining Loss: 0.304832\n",
      "Epoch: 19 \tTraining Loss: 0.292612\n",
      "Epoch: 20 \tTraining Loss: 0.280477\n",
      "Epoch: 21 \tTraining Loss: 0.269755\n",
      "Epoch: 22 \tTraining Loss: 0.259040\n",
      "Epoch: 23 \tTraining Loss: 0.249535\n",
      "Epoch: 24 \tTraining Loss: 0.241876\n",
      "Epoch: 25 \tTraining Loss: 0.233010\n",
      "Epoch: 26 \tTraining Loss: 0.224701\n",
      "Epoch: 27 \tTraining Loss: 0.217165\n",
      "Epoch: 28 \tTraining Loss: 0.211158\n",
      "Epoch: 29 \tTraining Loss: 0.203236\n",
      "Epoch: 30 \tTraining Loss: 0.194687\n",
      "Epoch: 31 \tTraining Loss: 0.188958\n",
      "Epoch: 32 \tTraining Loss: 0.180952\n",
      "Epoch: 33 \tTraining Loss: 0.175673\n",
      "Epoch: 34 \tTraining Loss: 0.168258\n",
      "Epoch: 35 \tTraining Loss: 0.160363\n",
      "Epoch: 36 \tTraining Loss: 0.159420\n",
      "Epoch: 37 \tTraining Loss: 0.155066\n",
      "Epoch: 38 \tTraining Loss: 0.147659\n",
      "Epoch: 39 \tTraining Loss: 0.141754\n",
      "Epoch: 40 \tTraining Loss: 0.138986\n",
      "Epoch: 41 \tTraining Loss: 0.136257\n",
      "Epoch: 42 \tTraining Loss: 0.130352\n",
      "Epoch: 43 \tTraining Loss: 0.125735\n",
      "Epoch: 44 \tTraining Loss: 0.122321\n",
      "Epoch: 45 \tTraining Loss: 0.118654\n",
      "Epoch: 46 \tTraining Loss: 0.115132\n",
      "Epoch: 47 \tTraining Loss: 0.112078\n",
      "Epoch: 48 \tTraining Loss: 0.107467\n",
      "Epoch: 49 \tTraining Loss: 0.104432\n",
      "Epoch: 50 \tTraining Loss: 0.105765\n",
      "Epoch: 51 \tTraining Loss: 0.103513\n",
      "Epoch: 52 \tTraining Loss: 0.099748\n",
      "Epoch: 53 \tTraining Loss: 0.094485\n",
      "Epoch: 54 \tTraining Loss: 0.091060\n",
      "Epoch: 55 \tTraining Loss: 0.085949\n",
      "Epoch: 56 \tTraining Loss: 0.083844\n",
      "Epoch: 57 \tTraining Loss: 0.084081\n",
      "Epoch: 58 \tTraining Loss: 0.083940\n",
      "Epoch: 59 \tTraining Loss: 0.084119\n",
      "Epoch: 60 \tTraining Loss: 0.081060\n",
      "Epoch: 61 \tTraining Loss: 0.080572\n",
      "Epoch: 62 \tTraining Loss: 0.075886\n",
      "Epoch: 63 \tTraining Loss: 0.072458\n",
      "Epoch: 64 \tTraining Loss: 0.070941\n",
      "Epoch: 65 \tTraining Loss: 0.070627\n",
      "Epoch: 66 \tTraining Loss: 0.068649\n",
      "Epoch: 67 \tTraining Loss: 0.068520\n",
      "Epoch: 68 \tTraining Loss: 0.069538\n",
      "Epoch: 69 \tTraining Loss: 0.066512\n",
      "Epoch: 70 \tTraining Loss: 0.065453\n",
      "Epoch: 71 \tTraining Loss: 0.063135\n",
      "Epoch: 72 \tTraining Loss: 0.061974\n",
      "Epoch: 73 \tTraining Loss: 0.063433\n",
      "Epoch: 74 \tTraining Loss: 0.060996\n",
      "Epoch: 75 \tTraining Loss: 0.058707\n",
      "Epoch: 76 \tTraining Loss: 0.055742\n",
      "Epoch: 77 \tTraining Loss: 0.053429\n",
      "Epoch: 78 \tTraining Loss: 0.054706\n",
      "Epoch: 79 \tTraining Loss: 0.055139\n",
      "Epoch: 80 \tTraining Loss: 0.053715\n",
      "Epoch: 81 \tTraining Loss: 0.054515\n",
      "Epoch: 82 \tTraining Loss: 0.055126\n",
      "Epoch: 83 \tTraining Loss: 0.054650\n",
      "Epoch: 84 \tTraining Loss: 0.054423\n",
      "Epoch: 85 \tTraining Loss: 0.055844\n",
      "Epoch: 86 \tTraining Loss: 0.050987\n",
      "Epoch: 87 \tTraining Loss: 0.046877\n",
      "Epoch: 88 \tTraining Loss: 0.046440\n",
      "Epoch: 89 \tTraining Loss: 0.045724\n",
      "Epoch: 90 \tTraining Loss: 0.043174\n",
      "Epoch: 91 \tTraining Loss: 0.044570\n",
      "Epoch: 92 \tTraining Loss: 0.044084\n",
      "Epoch: 93 \tTraining Loss: 0.044848\n",
      "Epoch: 94 \tTraining Loss: 0.043790\n",
      "Epoch: 95 \tTraining Loss: 0.044243\n",
      "Epoch: 96 \tTraining Loss: 0.046322\n",
      "Epoch: 97 \tTraining Loss: 0.045153\n",
      "Epoch: 98 \tTraining Loss: 0.043476\n",
      "Epoch: 99 \tTraining Loss: 0.044574\n",
      "Epoch: 100 \tTraining Loss: 0.045069\n",
      "FFNN Scores: \n",
      "Google model: \n",
      "Average Vector: \n",
      "Accuracy:  0.7361\n"
     ]
    }
   ],
   "source": [
    "X, Y = getAllReviewsConcatFeatures(df, google_w2v)\n",
    "\n",
    "train, test = getDatasetAndDataLoader(X, Y)\n",
    "\n",
    "classifier = train_ffnn_model(train, 3, 3000)\n",
    "\n",
    "actual, prediction = predict(classifier, test)\n",
    "\n",
    "accuracy = get_accuracy(actual, prediction)\n",
    "\n",
    "add_report_entry(\"FFNN\", \"Ternary\", \"Concat\", \"Pretrained\", accuracy)\n",
    "\n",
    "print(\"FFNN Scores: \")\n",
    "print(\"Google model: \")\n",
    "print(\"Average Vector: \")\n",
    "print(\"Accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Binary Classification Mean Vector (Custom W2V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.693985\n",
      "Epoch: 2 \tTraining Loss: 0.692407\n",
      "Epoch: 3 \tTraining Loss: 0.690630\n",
      "Epoch: 4 \tTraining Loss: 0.676900\n",
      "Epoch: 5 \tTraining Loss: 0.509735\n",
      "Epoch: 6 \tTraining Loss: 0.327292\n",
      "Epoch: 7 \tTraining Loss: 0.274400\n",
      "Epoch: 8 \tTraining Loss: 0.258408\n",
      "Epoch: 9 \tTraining Loss: 0.250640\n",
      "Epoch: 10 \tTraining Loss: 0.246103\n",
      "Epoch: 11 \tTraining Loss: 0.242816\n",
      "Epoch: 12 \tTraining Loss: 0.239970\n",
      "Epoch: 13 \tTraining Loss: 0.237517\n",
      "Epoch: 14 \tTraining Loss: 0.235152\n",
      "Epoch: 15 \tTraining Loss: 0.233216\n",
      "Epoch: 16 \tTraining Loss: 0.231195\n",
      "Epoch: 17 \tTraining Loss: 0.229397\n",
      "Epoch: 18 \tTraining Loss: 0.227720\n",
      "Epoch: 19 \tTraining Loss: 0.226091\n",
      "Epoch: 20 \tTraining Loss: 0.224625\n",
      "Epoch: 21 \tTraining Loss: 0.223275\n",
      "Epoch: 22 \tTraining Loss: 0.222011\n",
      "Epoch: 23 \tTraining Loss: 0.220729\n",
      "Epoch: 24 \tTraining Loss: 0.219540\n",
      "Epoch: 25 \tTraining Loss: 0.218470\n",
      "Epoch: 26 \tTraining Loss: 0.217391\n",
      "Epoch: 27 \tTraining Loss: 0.216410\n",
      "Epoch: 28 \tTraining Loss: 0.215495\n",
      "Epoch: 29 \tTraining Loss: 0.214546\n",
      "Epoch: 30 \tTraining Loss: 0.213709\n",
      "Epoch: 31 \tTraining Loss: 0.212831\n",
      "Epoch: 32 \tTraining Loss: 0.212030\n",
      "Epoch: 33 \tTraining Loss: 0.211265\n",
      "Epoch: 34 \tTraining Loss: 0.210533\n",
      "Epoch: 35 \tTraining Loss: 0.209817\n",
      "Epoch: 36 \tTraining Loss: 0.209142\n",
      "Epoch: 37 \tTraining Loss: 0.208463\n",
      "Epoch: 38 \tTraining Loss: 0.207935\n",
      "Epoch: 39 \tTraining Loss: 0.207305\n",
      "Epoch: 40 \tTraining Loss: 0.206714\n",
      "Epoch: 41 \tTraining Loss: 0.206112\n",
      "Epoch: 42 \tTraining Loss: 0.205533\n",
      "Epoch: 43 \tTraining Loss: 0.204951\n",
      "Epoch: 44 \tTraining Loss: 0.204386\n",
      "Epoch: 45 \tTraining Loss: 0.203848\n",
      "Epoch: 46 \tTraining Loss: 0.203347\n",
      "Epoch: 47 \tTraining Loss: 0.202789\n",
      "Epoch: 48 \tTraining Loss: 0.202279\n",
      "Epoch: 49 \tTraining Loss: 0.201864\n",
      "Epoch: 50 \tTraining Loss: 0.201329\n",
      "Epoch: 51 \tTraining Loss: 0.200901\n",
      "Epoch: 52 \tTraining Loss: 0.200416\n",
      "Epoch: 53 \tTraining Loss: 0.200019\n",
      "Epoch: 54 \tTraining Loss: 0.199601\n",
      "Epoch: 55 \tTraining Loss: 0.199152\n",
      "Epoch: 56 \tTraining Loss: 0.198739\n",
      "Epoch: 57 \tTraining Loss: 0.198349\n",
      "Epoch: 58 \tTraining Loss: 0.197943\n",
      "Epoch: 59 \tTraining Loss: 0.197586\n",
      "Epoch: 60 \tTraining Loss: 0.197194\n",
      "Epoch: 61 \tTraining Loss: 0.196824\n",
      "Epoch: 62 \tTraining Loss: 0.196413\n",
      "Epoch: 63 \tTraining Loss: 0.196054\n",
      "Epoch: 64 \tTraining Loss: 0.195737\n",
      "Epoch: 65 \tTraining Loss: 0.195361\n",
      "Epoch: 66 \tTraining Loss: 0.194997\n",
      "Epoch: 67 \tTraining Loss: 0.194676\n",
      "Epoch: 68 \tTraining Loss: 0.194368\n",
      "Epoch: 69 \tTraining Loss: 0.193952\n",
      "Epoch: 70 \tTraining Loss: 0.193680\n",
      "Epoch: 71 \tTraining Loss: 0.193358\n",
      "Epoch: 72 \tTraining Loss: 0.193065\n",
      "Epoch: 73 \tTraining Loss: 0.192717\n",
      "Epoch: 74 \tTraining Loss: 0.192348\n",
      "Epoch: 75 \tTraining Loss: 0.192119\n",
      "Epoch: 76 \tTraining Loss: 0.191789\n",
      "Epoch: 77 \tTraining Loss: 0.191514\n",
      "Epoch: 78 \tTraining Loss: 0.191242\n",
      "Epoch: 79 \tTraining Loss: 0.190947\n",
      "Epoch: 80 \tTraining Loss: 0.190668\n",
      "Epoch: 81 \tTraining Loss: 0.190324\n",
      "Epoch: 82 \tTraining Loss: 0.190100\n",
      "Epoch: 83 \tTraining Loss: 0.189873\n",
      "Epoch: 84 \tTraining Loss: 0.189631\n",
      "Epoch: 85 \tTraining Loss: 0.189395\n",
      "Epoch: 86 \tTraining Loss: 0.189139\n",
      "Epoch: 87 \tTraining Loss: 0.188883\n",
      "Epoch: 88 \tTraining Loss: 0.188589\n",
      "Epoch: 89 \tTraining Loss: 0.188329\n",
      "Epoch: 90 \tTraining Loss: 0.188166\n",
      "Epoch: 91 \tTraining Loss: 0.187871\n",
      "Epoch: 92 \tTraining Loss: 0.187571\n",
      "Epoch: 93 \tTraining Loss: 0.187377\n",
      "Epoch: 94 \tTraining Loss: 0.187104\n",
      "Epoch: 95 \tTraining Loss: 0.186879\n",
      "Epoch: 96 \tTraining Loss: 0.186643\n",
      "Epoch: 97 \tTraining Loss: 0.186402\n",
      "Epoch: 98 \tTraining Loss: 0.186179\n",
      "Epoch: 99 \tTraining Loss: 0.186005\n",
      "Epoch: 100 \tTraining Loss: 0.185740\n",
      "FFNN Scores: \n",
      "Google model: \n",
      "Average Vector: \n",
      "Accuracy:  0.922875\n"
     ]
    }
   ],
   "source": [
    "X, Y = filterNeutralReviewsMeanFeatures(df, custom_w2v)\n",
    "\n",
    "train, test = getDatasetAndDataLoader(X, Y)\n",
    "\n",
    "classifier = train_ffnn_model(train, 2)\n",
    "\n",
    "actual, prediction = predict(classifier, test)\n",
    "\n",
    "accuracy = get_accuracy(actual, prediction)\n",
    "\n",
    "add_report_entry(\"FFNN\", \"Binary\", \"Mean\", \"Custom\", accuracy)\n",
    "\n",
    "print(\"FFNN Scores: \")\n",
    "print(\"Google model: \")\n",
    "print(\"Average Vector: \")\n",
    "print(\"Accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ternary Classification Mean Vector (Custom W2V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 1.055723\n",
      "Epoch: 2 \tTraining Loss: 1.055381\n",
      "Epoch: 3 \tTraining Loss: 1.055116\n",
      "Epoch: 4 \tTraining Loss: 1.054350\n",
      "Epoch: 5 \tTraining Loss: 1.050653\n",
      "Epoch: 6 \tTraining Loss: 0.997074\n",
      "Epoch: 7 \tTraining Loss: 0.762368\n",
      "Epoch: 8 \tTraining Loss: 0.668196\n",
      "Epoch: 9 \tTraining Loss: 0.652143\n",
      "Epoch: 10 \tTraining Loss: 0.643720\n",
      "Epoch: 11 \tTraining Loss: 0.636172\n",
      "Epoch: 12 \tTraining Loss: 0.630059\n",
      "Epoch: 13 \tTraining Loss: 0.624016\n",
      "Epoch: 14 \tTraining Loss: 0.617206\n",
      "Epoch: 15 \tTraining Loss: 0.611068\n",
      "Epoch: 16 \tTraining Loss: 0.604781\n",
      "Epoch: 17 \tTraining Loss: 0.597926\n",
      "Epoch: 18 \tTraining Loss: 0.592089\n",
      "Epoch: 19 \tTraining Loss: 0.588060\n",
      "Epoch: 20 \tTraining Loss: 0.584192\n",
      "Epoch: 21 \tTraining Loss: 0.581027\n",
      "Epoch: 22 \tTraining Loss: 0.578001\n",
      "Epoch: 23 \tTraining Loss: 0.575277\n",
      "Epoch: 24 \tTraining Loss: 0.572727\n",
      "Epoch: 25 \tTraining Loss: 0.570494\n",
      "Epoch: 26 \tTraining Loss: 0.568179\n",
      "Epoch: 27 \tTraining Loss: 0.566009\n",
      "Epoch: 28 \tTraining Loss: 0.563898\n",
      "Epoch: 29 \tTraining Loss: 0.561793\n",
      "Epoch: 30 \tTraining Loss: 0.559542\n",
      "Epoch: 31 \tTraining Loss: 0.557261\n",
      "Epoch: 32 \tTraining Loss: 0.554032\n",
      "Epoch: 33 \tTraining Loss: 0.548488\n",
      "Epoch: 34 \tTraining Loss: 0.538226\n",
      "Epoch: 35 \tTraining Loss: 0.524192\n",
      "Epoch: 36 \tTraining Loss: 0.514029\n",
      "Epoch: 37 \tTraining Loss: 0.507693\n",
      "Epoch: 38 \tTraining Loss: 0.502882\n",
      "Epoch: 39 \tTraining Loss: 0.498971\n",
      "Epoch: 40 \tTraining Loss: 0.495656\n",
      "Epoch: 41 \tTraining Loss: 0.492722\n",
      "Epoch: 42 \tTraining Loss: 0.490254\n",
      "Epoch: 43 \tTraining Loss: 0.488054\n",
      "Epoch: 44 \tTraining Loss: 0.486181\n",
      "Epoch: 45 \tTraining Loss: 0.484525\n",
      "Epoch: 46 \tTraining Loss: 0.482916\n",
      "Epoch: 47 \tTraining Loss: 0.481502\n",
      "Epoch: 48 \tTraining Loss: 0.480103\n",
      "Epoch: 49 \tTraining Loss: 0.478853\n",
      "Epoch: 50 \tTraining Loss: 0.477544\n",
      "Epoch: 51 \tTraining Loss: 0.476339\n",
      "Epoch: 52 \tTraining Loss: 0.475201\n",
      "Epoch: 53 \tTraining Loss: 0.474165\n",
      "Epoch: 54 \tTraining Loss: 0.473234\n",
      "Epoch: 55 \tTraining Loss: 0.472378\n",
      "Epoch: 56 \tTraining Loss: 0.471482\n",
      "Epoch: 57 \tTraining Loss: 0.470672\n",
      "Epoch: 58 \tTraining Loss: 0.469791\n",
      "Epoch: 59 \tTraining Loss: 0.469103\n",
      "Epoch: 60 \tTraining Loss: 0.468336\n",
      "Epoch: 61 \tTraining Loss: 0.467628\n",
      "Epoch: 62 \tTraining Loss: 0.466947\n",
      "Epoch: 63 \tTraining Loss: 0.466354\n",
      "Epoch: 64 \tTraining Loss: 0.465669\n",
      "Epoch: 65 \tTraining Loss: 0.465080\n",
      "Epoch: 66 \tTraining Loss: 0.464521\n",
      "Epoch: 67 \tTraining Loss: 0.463896\n",
      "Epoch: 68 \tTraining Loss: 0.463339\n",
      "Epoch: 69 \tTraining Loss: 0.462830\n",
      "Epoch: 70 \tTraining Loss: 0.462288\n",
      "Epoch: 71 \tTraining Loss: 0.461730\n",
      "Epoch: 72 \tTraining Loss: 0.461143\n",
      "Epoch: 73 \tTraining Loss: 0.460622\n",
      "Epoch: 74 \tTraining Loss: 0.460127\n",
      "Epoch: 75 \tTraining Loss: 0.459595\n",
      "Epoch: 76 \tTraining Loss: 0.459068\n",
      "Epoch: 77 \tTraining Loss: 0.458618\n",
      "Epoch: 78 \tTraining Loss: 0.458072\n",
      "Epoch: 79 \tTraining Loss: 0.457672\n",
      "Epoch: 80 \tTraining Loss: 0.457266\n",
      "Epoch: 81 \tTraining Loss: 0.456802\n",
      "Epoch: 82 \tTraining Loss: 0.456392\n",
      "Epoch: 83 \tTraining Loss: 0.456027\n",
      "Epoch: 84 \tTraining Loss: 0.455620\n",
      "Epoch: 85 \tTraining Loss: 0.455187\n",
      "Epoch: 86 \tTraining Loss: 0.454795\n",
      "Epoch: 87 \tTraining Loss: 0.454343\n",
      "Epoch: 88 \tTraining Loss: 0.453984\n",
      "Epoch: 89 \tTraining Loss: 0.453545\n",
      "Epoch: 90 \tTraining Loss: 0.453169\n",
      "Epoch: 91 \tTraining Loss: 0.452869\n",
      "Epoch: 92 \tTraining Loss: 0.452425\n",
      "Epoch: 93 \tTraining Loss: 0.452048\n",
      "Epoch: 94 \tTraining Loss: 0.451710\n",
      "Epoch: 95 \tTraining Loss: 0.451306\n",
      "Epoch: 96 \tTraining Loss: 0.451032\n",
      "Epoch: 97 \tTraining Loss: 0.450675\n",
      "Epoch: 98 \tTraining Loss: 0.450312\n",
      "Epoch: 99 \tTraining Loss: 0.449935\n",
      "Epoch: 100 \tTraining Loss: 0.449587\n",
      "FFNN Scores: \n",
      "Google model: \n",
      "Average Vector: \n",
      "Accuracy:  0.80384\n"
     ]
    }
   ],
   "source": [
    "X, Y = getAllReviewsMeanFeatures(df, custom_w2v)\n",
    "\n",
    "train, test = getDatasetAndDataLoader(X, Y)\n",
    "\n",
    "classifier = train_ffnn_model(train, 3)\n",
    "\n",
    "actual, prediction = predict(classifier, test)\n",
    "\n",
    "accuracy = get_accuracy(actual, prediction)\n",
    "\n",
    "add_report_entry(\"FFNN\", \"Ternary\", \"Mean\", \"Custom\", accuracy)\n",
    "\n",
    "print(\"FFNN Scores: \")\n",
    "print(\"Google model: \")\n",
    "print(\"Average Vector: \")\n",
    "print(\"Accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Binary Classification with Concat Vector (Custom W2V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.292561\n",
      "Epoch: 2 \tTraining Loss: 0.220199\n",
      "Epoch: 3 \tTraining Loss: 0.200544\n",
      "Epoch: 4 \tTraining Loss: 0.184193\n",
      "Epoch: 5 \tTraining Loss: 0.168443\n",
      "Epoch: 6 \tTraining Loss: 0.153185\n",
      "Epoch: 7 \tTraining Loss: 0.137913\n",
      "Epoch: 8 \tTraining Loss: 0.122785\n",
      "Epoch: 9 \tTraining Loss: 0.108976\n",
      "Epoch: 10 \tTraining Loss: 0.097310\n",
      "Epoch: 11 \tTraining Loss: 0.089066\n",
      "Epoch: 12 \tTraining Loss: 0.083358\n",
      "Epoch: 13 \tTraining Loss: 0.075546\n",
      "Epoch: 14 \tTraining Loss: 0.068061\n",
      "Epoch: 15 \tTraining Loss: 0.061479\n",
      "Epoch: 16 \tTraining Loss: 0.058741\n",
      "Epoch: 17 \tTraining Loss: 0.053324\n",
      "Epoch: 18 \tTraining Loss: 0.047577\n",
      "Epoch: 19 \tTraining Loss: 0.043699\n",
      "Epoch: 20 \tTraining Loss: 0.041699\n",
      "Epoch: 21 \tTraining Loss: 0.036995\n",
      "Epoch: 22 \tTraining Loss: 0.034438\n",
      "Epoch: 23 \tTraining Loss: 0.032354\n",
      "Epoch: 24 \tTraining Loss: 0.029490\n",
      "Epoch: 25 \tTraining Loss: 0.026532\n",
      "Epoch: 26 \tTraining Loss: 0.023851\n",
      "Epoch: 27 \tTraining Loss: 0.022361\n",
      "Epoch: 28 \tTraining Loss: 0.022567\n",
      "Epoch: 29 \tTraining Loss: 0.019890\n",
      "Epoch: 30 \tTraining Loss: 0.016151\n",
      "Epoch: 31 \tTraining Loss: 0.017825\n",
      "Epoch: 32 \tTraining Loss: 0.017341\n",
      "Epoch: 33 \tTraining Loss: 0.017985\n",
      "Epoch: 34 \tTraining Loss: 0.015503\n",
      "Epoch: 35 \tTraining Loss: 0.012953\n",
      "Epoch: 36 \tTraining Loss: 0.011646\n",
      "Epoch: 37 \tTraining Loss: 0.010936\n",
      "Epoch: 38 \tTraining Loss: 0.010203\n",
      "Epoch: 39 \tTraining Loss: 0.009719\n",
      "Epoch: 40 \tTraining Loss: 0.011224\n",
      "Epoch: 41 \tTraining Loss: 0.012661\n",
      "Epoch: 42 \tTraining Loss: 0.012965\n",
      "Epoch: 43 \tTraining Loss: 0.010295\n",
      "Epoch: 44 \tTraining Loss: 0.008531\n",
      "Epoch: 45 \tTraining Loss: 0.007188\n",
      "Epoch: 46 \tTraining Loss: 0.006549\n",
      "Epoch: 47 \tTraining Loss: 0.004437\n",
      "Epoch: 48 \tTraining Loss: 0.002766\n",
      "Epoch: 49 \tTraining Loss: 0.001535\n",
      "Epoch: 50 \tTraining Loss: 0.001304\n",
      "Epoch: 51 \tTraining Loss: 0.001149\n",
      "Epoch: 52 \tTraining Loss: 0.000981\n",
      "Epoch: 53 \tTraining Loss: 0.001023\n",
      "Epoch: 54 \tTraining Loss: 0.001008\n",
      "Epoch: 55 \tTraining Loss: 0.000946\n",
      "Epoch: 56 \tTraining Loss: 0.000863\n",
      "Epoch: 57 \tTraining Loss: 0.000859\n",
      "Epoch: 58 \tTraining Loss: 0.000813\n",
      "Epoch: 59 \tTraining Loss: 0.000859\n",
      "Epoch: 60 \tTraining Loss: 0.000835\n",
      "Epoch: 61 \tTraining Loss: 0.000804\n",
      "Epoch: 62 \tTraining Loss: 0.000789\n",
      "Epoch: 63 \tTraining Loss: 0.000760\n",
      "Epoch: 64 \tTraining Loss: 0.000749\n",
      "Epoch: 65 \tTraining Loss: 0.000739\n",
      "Epoch: 66 \tTraining Loss: 0.000727\n",
      "Epoch: 67 \tTraining Loss: 0.000711\n",
      "Epoch: 68 \tTraining Loss: 0.000703\n",
      "Epoch: 69 \tTraining Loss: 0.000711\n",
      "Epoch: 70 \tTraining Loss: 0.000689\n",
      "Epoch: 71 \tTraining Loss: 0.000691\n",
      "Epoch: 72 \tTraining Loss: 0.000675\n",
      "Epoch: 73 \tTraining Loss: 0.000659\n",
      "Epoch: 74 \tTraining Loss: 0.000658\n",
      "Epoch: 75 \tTraining Loss: 0.000655\n",
      "Epoch: 76 \tTraining Loss: 0.000643\n",
      "Epoch: 77 \tTraining Loss: 0.000633\n",
      "Epoch: 78 \tTraining Loss: 0.000638\n",
      "Epoch: 79 \tTraining Loss: 0.000639\n",
      "Epoch: 80 \tTraining Loss: 0.000620\n",
      "Epoch: 81 \tTraining Loss: 0.000619\n",
      "Epoch: 82 \tTraining Loss: 0.000613\n",
      "Epoch: 83 \tTraining Loss: 0.000612\n",
      "Epoch: 84 \tTraining Loss: 0.000610\n",
      "Epoch: 85 \tTraining Loss: 0.000605\n",
      "Epoch: 86 \tTraining Loss: 0.000612\n",
      "Epoch: 87 \tTraining Loss: 0.000601\n",
      "Epoch: 88 \tTraining Loss: 0.000607\n",
      "Epoch: 89 \tTraining Loss: 0.000599\n",
      "Epoch: 90 \tTraining Loss: 0.000610\n",
      "Epoch: 91 \tTraining Loss: 0.000592\n",
      "Epoch: 92 \tTraining Loss: 0.000582\n",
      "Epoch: 93 \tTraining Loss: 0.000571\n",
      "Epoch: 94 \tTraining Loss: 0.000568\n",
      "Epoch: 95 \tTraining Loss: 0.000573\n",
      "Epoch: 96 \tTraining Loss: 0.000567\n",
      "Epoch: 97 \tTraining Loss: 0.000566\n",
      "Epoch: 98 \tTraining Loss: 0.000558\n",
      "Epoch: 99 \tTraining Loss: 0.000555\n",
      "Epoch: 100 \tTraining Loss: 0.000557\n",
      "FFNN Scores: \n",
      "Google model: \n",
      "Average Vector: \n",
      "Accuracy:  0.892375\n"
     ]
    }
   ],
   "source": [
    "X, Y = filterNeutralReviewsConcatFeatures(df, custom_w2v)\n",
    "\n",
    "train, test = getDatasetAndDataLoader(X, Y)\n",
    "\n",
    "classifier = train_ffnn_model(train, 2, 3000)\n",
    "\n",
    "actual, prediction = predict(classifier, test)\n",
    "\n",
    "accuracy = get_accuracy(actual, prediction)\n",
    "\n",
    "add_report_entry(\"FFNN\", \"Binary\", \"Concat\", \"Custom\", accuracy)\n",
    "\n",
    "print(\"FFNN Scores: \")\n",
    "print(\"Google model: \")\n",
    "print(\"Average Vector: \")\n",
    "print(\"Accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ternary Classification with Concat Vector (Custom W2V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.574646\n",
      "Epoch: 2 \tTraining Loss: 0.475269\n",
      "Epoch: 3 \tTraining Loss: 0.451373\n",
      "Epoch: 4 \tTraining Loss: 0.432924\n",
      "Epoch: 5 \tTraining Loss: 0.416508\n",
      "Epoch: 6 \tTraining Loss: 0.400882\n",
      "Epoch: 7 \tTraining Loss: 0.385926\n",
      "Epoch: 8 \tTraining Loss: 0.371548\n",
      "Epoch: 9 \tTraining Loss: 0.357348\n",
      "Epoch: 10 \tTraining Loss: 0.344097\n",
      "Epoch: 11 \tTraining Loss: 0.330990\n",
      "Epoch: 12 \tTraining Loss: 0.318943\n",
      "Epoch: 13 \tTraining Loss: 0.307344\n",
      "Epoch: 14 \tTraining Loss: 0.297223\n",
      "Epoch: 15 \tTraining Loss: 0.287606\n",
      "Epoch: 16 \tTraining Loss: 0.278713\n",
      "Epoch: 17 \tTraining Loss: 0.270143\n",
      "Epoch: 18 \tTraining Loss: 0.263040\n",
      "Epoch: 19 \tTraining Loss: 0.256589\n",
      "Epoch: 20 \tTraining Loss: 0.250314\n",
      "Epoch: 21 \tTraining Loss: 0.242613\n",
      "Epoch: 22 \tTraining Loss: 0.236626\n",
      "Epoch: 23 \tTraining Loss: 0.232268\n",
      "Epoch: 24 \tTraining Loss: 0.226819\n",
      "Epoch: 25 \tTraining Loss: 0.222134\n",
      "Epoch: 26 \tTraining Loss: 0.215604\n",
      "Epoch: 27 \tTraining Loss: 0.210329\n",
      "Epoch: 28 \tTraining Loss: 0.205621\n",
      "Epoch: 29 \tTraining Loss: 0.203656\n",
      "Epoch: 30 \tTraining Loss: 0.197775\n",
      "Epoch: 31 \tTraining Loss: 0.193257\n",
      "Epoch: 32 \tTraining Loss: 0.190542\n",
      "Epoch: 33 \tTraining Loss: 0.186878\n",
      "Epoch: 34 \tTraining Loss: 0.181336\n",
      "Epoch: 35 \tTraining Loss: 0.178689\n",
      "Epoch: 36 \tTraining Loss: 0.176300\n",
      "Epoch: 37 \tTraining Loss: 0.171533\n",
      "Epoch: 38 \tTraining Loss: 0.168831\n",
      "Epoch: 39 \tTraining Loss: 0.167779\n",
      "Epoch: 40 \tTraining Loss: 0.167303\n",
      "Epoch: 41 \tTraining Loss: 0.160378\n",
      "Epoch: 42 \tTraining Loss: 0.155392\n",
      "Epoch: 43 \tTraining Loss: 0.153403\n",
      "Epoch: 44 \tTraining Loss: 0.151968\n",
      "Epoch: 45 \tTraining Loss: 0.149919\n",
      "Epoch: 46 \tTraining Loss: 0.146396\n",
      "Epoch: 47 \tTraining Loss: 0.145179\n",
      "Epoch: 48 \tTraining Loss: 0.141456\n",
      "Epoch: 49 \tTraining Loss: 0.140903\n",
      "Epoch: 50 \tTraining Loss: 0.139990\n",
      "Epoch: 51 \tTraining Loss: 0.136990\n",
      "Epoch: 52 \tTraining Loss: 0.135960\n",
      "Epoch: 53 \tTraining Loss: 0.133646\n",
      "Epoch: 54 \tTraining Loss: 0.131520\n",
      "Epoch: 55 \tTraining Loss: 0.128092\n",
      "Epoch: 56 \tTraining Loss: 0.125082\n",
      "Epoch: 57 \tTraining Loss: 0.125935\n",
      "Epoch: 58 \tTraining Loss: 0.124645\n",
      "Epoch: 59 \tTraining Loss: 0.120921\n",
      "Epoch: 60 \tTraining Loss: 0.117217\n",
      "Epoch: 61 \tTraining Loss: 0.119197\n",
      "Epoch: 62 \tTraining Loss: 0.121310\n",
      "Epoch: 63 \tTraining Loss: 0.115982\n",
      "Epoch: 64 \tTraining Loss: 0.113411\n",
      "Epoch: 65 \tTraining Loss: 0.113576\n",
      "Epoch: 66 \tTraining Loss: 0.112071\n",
      "Epoch: 67 \tTraining Loss: 0.110086\n",
      "Epoch: 68 \tTraining Loss: 0.109646\n",
      "Epoch: 69 \tTraining Loss: 0.106648\n",
      "Epoch: 70 \tTraining Loss: 0.106510\n",
      "Epoch: 71 \tTraining Loss: 0.108165\n",
      "Epoch: 72 \tTraining Loss: 0.108054\n",
      "Epoch: 73 \tTraining Loss: 0.100829\n",
      "Epoch: 74 \tTraining Loss: 0.099115\n",
      "Epoch: 75 \tTraining Loss: 0.098847\n",
      "Epoch: 76 \tTraining Loss: 0.101597\n",
      "Epoch: 77 \tTraining Loss: 0.101453\n",
      "Epoch: 78 \tTraining Loss: 0.096494\n",
      "Epoch: 79 \tTraining Loss: 0.094398\n",
      "Epoch: 80 \tTraining Loss: 0.096294\n",
      "Epoch: 81 \tTraining Loss: 0.095761\n",
      "Epoch: 82 \tTraining Loss: 0.101030\n",
      "Epoch: 83 \tTraining Loss: 0.093399\n",
      "Epoch: 84 \tTraining Loss: 0.089642\n",
      "Epoch: 85 \tTraining Loss: 0.091518\n",
      "Epoch: 86 \tTraining Loss: 0.087993\n",
      "Epoch: 87 \tTraining Loss: 0.087994\n",
      "Epoch: 88 \tTraining Loss: 0.090327\n",
      "Epoch: 89 \tTraining Loss: 0.088483\n",
      "Epoch: 90 \tTraining Loss: 0.086885\n",
      "Epoch: 91 \tTraining Loss: 0.083474\n",
      "Epoch: 92 \tTraining Loss: 0.086457\n",
      "Epoch: 93 \tTraining Loss: 0.085396\n",
      "Epoch: 94 \tTraining Loss: 0.085211\n",
      "Epoch: 95 \tTraining Loss: 0.086131\n",
      "Epoch: 96 \tTraining Loss: 0.083555\n",
      "Epoch: 97 \tTraining Loss: 0.082512\n",
      "Epoch: 98 \tTraining Loss: 0.085095\n",
      "Epoch: 99 \tTraining Loss: 0.083032\n",
      "Epoch: 100 \tTraining Loss: 0.078035\n",
      "FFNN Scores: \n",
      "Google model: \n",
      "Average Vector: \n",
      "Accuracy:  0.75066\n"
     ]
    }
   ],
   "source": [
    "X, Y = getAllReviewsConcatFeatures(df, custom_w2v)\n",
    "\n",
    "train, test = getDatasetAndDataLoader(X, Y)\n",
    "\n",
    "classifier = train_ffnn_model(train, 3, 3000)\n",
    "\n",
    "actual, prediction = predict(classifier, test)\n",
    "\n",
    "accuracy = get_accuracy(actual, prediction)\n",
    "\n",
    "add_report_entry(\"FFNN\", \"Ternary\", \"Concat\", \"Custom\", accuracy)\n",
    "\n",
    "print(\"FFNN Scores: \")\n",
    "print(\"Google model: \")\n",
    "print(\"Average Vector: \")\n",
    "print(\"Accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomCNN(nn.Module):\n",
    "    def __init__(self, input_length, embedding_features, output_features):\n",
    "        super(CustomCNN, self).__init__()\n",
    "        layer1_features = 50\n",
    "        layer2_features = 10\n",
    "\n",
    "        self.output_features = output_features\n",
    "\n",
    "        self.conv1 = nn.Conv1d(\n",
    "            in_channels=embedding_features,\n",
    "            out_channels=layer1_features,\n",
    "            kernel_size=3,\n",
    "            padding=1\n",
    "        )\n",
    "\n",
    "        self.pool1 = nn.MaxPool1d(kernel_size=2)\n",
    "\n",
    "        self.conv2 = nn.Conv1d(\n",
    "            in_channels=layer1_features,\n",
    "            out_channels=layer2_features,\n",
    "            kernel_size=3,\n",
    "            padding=1\n",
    "        )\n",
    "\n",
    "        self.pool2 = nn.MaxPool1d(kernel_size=2)\n",
    "\n",
    "        self.output = nn.Linear(\n",
    "            in_features=layer2_features * (input_length // 4),\n",
    "            out_features=output_features\n",
    "        )\n",
    "\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 2, 1) \n",
    "\n",
    "        x = self.conv1(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        x = self.pool1(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        x = self.pool2(x)\n",
    "\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        x = self.output(x)\n",
    "\n",
    "        if self.output_features == 2:\n",
    "            x = self.sigmoid(x)\n",
    "        else:\n",
    "            x = self.softmax(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_cnn_model(dataloader, input_length = 50, embedding_features=300, output_features=2):\n",
    "    model = CustomCNN(input_length, embedding_features, output_features)\n",
    "\n",
    "    learning_rate = 0.003\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n",
    "    no_epochs = 100\n",
    "\n",
    "    for epoch in range(no_epochs):\n",
    "        train_loss = 0.0\n",
    "\n",
    "        model.train()\n",
    "\n",
    "        for data, target in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            output = model(data)\n",
    "\n",
    "            loss = criterion(output, target - 1)\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item() * data.size(0)\n",
    "\n",
    "        train_loss = train_loss/len(dataloader.dataset)\n",
    "\n",
    "        print(\n",
    "            'Epoch: {} \\tTraining Loss: {:.6f}'.format(\n",
    "                epoch+1,\n",
    "                train_loss,\n",
    "            )\n",
    "        )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCNNVectorEmbedding(tokens_series, w2v_model):\n",
    "  word_embeddings = []\n",
    "  for tokens_list in tokens_series:\n",
    "    tokens = []\n",
    "    \n",
    "    if len(tokens_list) > 50:\n",
    "      tokens = tokens_list[:50]\n",
    "    else:\n",
    "      current_len = len(tokens_list)\n",
    "      remaining_len = 50 - current_len\n",
    "      remaining_list = [''] * remaining_len\n",
    "      tokens = tokens_list + remaining_list\n",
    "\n",
    "    current_embedding = []\n",
    "    for token in tokens:\n",
    "      if token in w2v_model:\n",
    "        current_embedding.append(w2v_model[token])\n",
    "      else:\n",
    "        current_embedding.append(np.zeros(300))\n",
    "    \n",
    "    word_embeddings.append(current_embedding)\n",
    "\n",
    "  return torch.tensor(word_embeddings, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCNNDataLoader(X, Y):\n",
    "    X_train, X_test, Y_train, Y_test = getTrainTestSplit(X, Y)\n",
    "\n",
    "    train_dataset = CustomDataset(X_train, Y_train)\n",
    "    test_dataset = CustomDataset(X_test, Y_test)\n",
    "    \n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=False)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "    return train_dataloader, test_dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNN Binary Classification (Pretrained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.676508\n",
      "Epoch: 2 \tTraining Loss: 0.486140\n",
      "Epoch: 3 \tTraining Loss: 0.413393\n",
      "Epoch: 4 \tTraining Loss: 0.400295\n",
      "Epoch: 5 \tTraining Loss: 0.394057\n",
      "Epoch: 6 \tTraining Loss: 0.389876\n",
      "Epoch: 7 \tTraining Loss: 0.386685\n",
      "Epoch: 8 \tTraining Loss: 0.384098\n",
      "Epoch: 9 \tTraining Loss: 0.381912\n",
      "Epoch: 10 \tTraining Loss: 0.379989\n",
      "Epoch: 11 \tTraining Loss: 0.378261\n",
      "Epoch: 12 \tTraining Loss: 0.376696\n",
      "Epoch: 13 \tTraining Loss: 0.375267\n",
      "Epoch: 14 \tTraining Loss: 0.373947\n",
      "Epoch: 15 \tTraining Loss: 0.372668\n",
      "Epoch: 16 \tTraining Loss: 0.371482\n",
      "Epoch: 17 \tTraining Loss: 0.370360\n",
      "Epoch: 18 \tTraining Loss: 0.369312\n",
      "Epoch: 19 \tTraining Loss: 0.368312\n",
      "Epoch: 20 \tTraining Loss: 0.367360\n",
      "Epoch: 21 \tTraining Loss: 0.366435\n",
      "Epoch: 22 \tTraining Loss: 0.365532\n",
      "Epoch: 23 \tTraining Loss: 0.364651\n",
      "Epoch: 24 \tTraining Loss: 0.363822\n",
      "Epoch: 25 \tTraining Loss: 0.363022\n",
      "Epoch: 26 \tTraining Loss: 0.362257\n",
      "Epoch: 27 \tTraining Loss: 0.361524\n",
      "Epoch: 28 \tTraining Loss: 0.360855\n",
      "Epoch: 29 \tTraining Loss: 0.360201\n",
      "Epoch: 30 \tTraining Loss: 0.359568\n",
      "Epoch: 31 \tTraining Loss: 0.358950\n",
      "Epoch: 32 \tTraining Loss: 0.358355\n",
      "Epoch: 33 \tTraining Loss: 0.357785\n",
      "Epoch: 34 \tTraining Loss: 0.357234\n",
      "Epoch: 35 \tTraining Loss: 0.356708\n",
      "Epoch: 36 \tTraining Loss: 0.356226\n",
      "Epoch: 37 \tTraining Loss: 0.355833\n",
      "Epoch: 38 \tTraining Loss: 0.355529\n",
      "Epoch: 39 \tTraining Loss: 0.355322\n",
      "Epoch: 40 \tTraining Loss: 0.355108\n",
      "Epoch: 41 \tTraining Loss: 0.354958\n",
      "Epoch: 42 \tTraining Loss: 0.354808\n",
      "Epoch: 43 \tTraining Loss: 0.354827\n",
      "Epoch: 44 \tTraining Loss: 0.355148\n",
      "Epoch: 45 \tTraining Loss: 0.355277\n",
      "Epoch: 46 \tTraining Loss: 0.355032\n",
      "Epoch: 47 \tTraining Loss: 0.355443\n",
      "Epoch: 48 \tTraining Loss: 0.355861\n",
      "Epoch: 49 \tTraining Loss: 0.356641\n",
      "Epoch: 50 \tTraining Loss: 0.356170\n",
      "Epoch: 51 \tTraining Loss: 0.355952\n",
      "Epoch: 52 \tTraining Loss: 0.355345\n",
      "Epoch: 53 \tTraining Loss: 0.354645\n",
      "Epoch: 54 \tTraining Loss: 0.353987\n",
      "Epoch: 55 \tTraining Loss: 0.352903\n",
      "Epoch: 56 \tTraining Loss: 0.352199\n",
      "Epoch: 57 \tTraining Loss: 0.351815\n",
      "Epoch: 58 \tTraining Loss: 0.351259\n",
      "Epoch: 59 \tTraining Loss: 0.350942\n",
      "Epoch: 60 \tTraining Loss: 0.350626\n",
      "Epoch: 61 \tTraining Loss: 0.350232\n",
      "Epoch: 62 \tTraining Loss: 0.349769\n",
      "Epoch: 63 \tTraining Loss: 0.349378\n",
      "Epoch: 64 \tTraining Loss: 0.349075\n",
      "Epoch: 65 \tTraining Loss: 0.349078\n",
      "Epoch: 66 \tTraining Loss: 0.349034\n",
      "Epoch: 67 \tTraining Loss: 0.348726\n",
      "Epoch: 68 \tTraining Loss: 0.348573\n",
      "Epoch: 69 \tTraining Loss: 0.348623\n",
      "Epoch: 70 \tTraining Loss: 0.348623\n",
      "Epoch: 71 \tTraining Loss: 0.348223\n",
      "Epoch: 72 \tTraining Loss: 0.347577\n",
      "Epoch: 73 \tTraining Loss: 0.346913\n",
      "Epoch: 74 \tTraining Loss: 0.346250\n",
      "Epoch: 75 \tTraining Loss: 0.345813\n",
      "Epoch: 76 \tTraining Loss: 0.345459\n",
      "Epoch: 77 \tTraining Loss: 0.345008\n",
      "Epoch: 78 \tTraining Loss: 0.344713\n",
      "Epoch: 79 \tTraining Loss: 0.344433\n",
      "Epoch: 80 \tTraining Loss: 0.344334\n",
      "Epoch: 81 \tTraining Loss: 0.344204\n",
      "Epoch: 82 \tTraining Loss: 0.344017\n",
      "Epoch: 83 \tTraining Loss: 0.343780\n",
      "Epoch: 84 \tTraining Loss: 0.343643\n",
      "Epoch: 85 \tTraining Loss: 0.343336\n",
      "Epoch: 86 \tTraining Loss: 0.343045\n",
      "Epoch: 87 \tTraining Loss: 0.342832\n",
      "Epoch: 88 \tTraining Loss: 0.342772\n",
      "Epoch: 89 \tTraining Loss: 0.342673\n",
      "Epoch: 90 \tTraining Loss: 0.342604\n",
      "Epoch: 91 \tTraining Loss: 0.342620\n",
      "Epoch: 92 \tTraining Loss: 0.342700\n",
      "Epoch: 93 \tTraining Loss: 0.342649\n",
      "Epoch: 94 \tTraining Loss: 0.342789\n",
      "Epoch: 95 \tTraining Loss: 0.342679\n",
      "Epoch: 96 \tTraining Loss: 0.342526\n",
      "Epoch: 97 \tTraining Loss: 0.342349\n",
      "Epoch: 98 \tTraining Loss: 0.341953\n",
      "Epoch: 99 \tTraining Loss: 0.341555\n",
      "Epoch: 100 \tTraining Loss: 0.341289\n",
      "0.93145\n"
     ]
    }
   ],
   "source": [
    "X = df.loc[df[\"sentiment_score\"] != 3, \"tokenised_review_text\"]\n",
    "X = getCNNVectorEmbedding(X, google_w2v)\n",
    "Y = df.loc[df[\"sentiment_score\"] != 3, \"sentiment_score\"]\n",
    "Y = torch.tensor(Y, dtype=torch.long)\n",
    "\n",
    "train, test = getCNNDataLoader(X, Y)\n",
    "\n",
    "classifier = train_cnn_model(train)\n",
    "\n",
    "actual, prediction = predict(classifier, test)\n",
    "\n",
    "accuracy = get_accuracy(actual, prediction)\n",
    "\n",
    "add_report_entry(\"CNN\", \"Binary\", \"50x300\", \"Pretrained\", accuracy)\n",
    "\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNN Binary Classification (Custom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.437417\n",
      "Epoch: 2 \tTraining Loss: 0.382830\n",
      "Epoch: 3 \tTraining Loss: 0.376285\n",
      "Epoch: 4 \tTraining Loss: 0.372148\n",
      "Epoch: 5 \tTraining Loss: 0.369024\n",
      "Epoch: 6 \tTraining Loss: 0.366459\n",
      "Epoch: 7 \tTraining Loss: 0.364244\n",
      "Epoch: 8 \tTraining Loss: 0.362228\n",
      "Epoch: 9 \tTraining Loss: 0.360408\n",
      "Epoch: 10 \tTraining Loss: 0.358835\n",
      "Epoch: 11 \tTraining Loss: 0.357646\n",
      "Epoch: 12 \tTraining Loss: 0.356532\n",
      "Epoch: 13 \tTraining Loss: 0.355769\n",
      "Epoch: 14 \tTraining Loss: 0.355216\n",
      "Epoch: 15 \tTraining Loss: 0.354662\n",
      "Epoch: 16 \tTraining Loss: 0.353713\n",
      "Epoch: 17 \tTraining Loss: 0.352966\n",
      "Epoch: 18 \tTraining Loss: 0.352665\n",
      "Epoch: 19 \tTraining Loss: 0.351545\n",
      "Epoch: 20 \tTraining Loss: 0.351643\n",
      "Epoch: 21 \tTraining Loss: 0.350668\n",
      "Epoch: 22 \tTraining Loss: 0.350253\n",
      "Epoch: 23 \tTraining Loss: 0.349516\n",
      "Epoch: 24 \tTraining Loss: 0.348925\n",
      "Epoch: 25 \tTraining Loss: 0.348492\n",
      "Epoch: 26 \tTraining Loss: 0.348157\n",
      "Epoch: 27 \tTraining Loss: 0.347545\n",
      "Epoch: 28 \tTraining Loss: 0.347262\n",
      "Epoch: 29 \tTraining Loss: 0.346661\n",
      "Epoch: 30 \tTraining Loss: 0.346506\n",
      "Epoch: 31 \tTraining Loss: 0.346197\n",
      "Epoch: 32 \tTraining Loss: 0.345736\n",
      "Epoch: 33 \tTraining Loss: 0.345154\n",
      "Epoch: 34 \tTraining Loss: 0.345369\n",
      "Epoch: 35 \tTraining Loss: 0.344822\n",
      "Epoch: 36 \tTraining Loss: 0.344321\n",
      "Epoch: 37 \tTraining Loss: 0.343836\n",
      "Epoch: 38 \tTraining Loss: 0.343847\n",
      "Epoch: 39 \tTraining Loss: 0.343741\n",
      "Epoch: 40 \tTraining Loss: 0.343265\n",
      "Epoch: 41 \tTraining Loss: 0.342645\n",
      "Epoch: 42 \tTraining Loss: 0.342550\n",
      "Epoch: 43 \tTraining Loss: 0.341923\n",
      "Epoch: 44 \tTraining Loss: 0.342076\n",
      "Epoch: 45 \tTraining Loss: 0.341592\n",
      "Epoch: 46 \tTraining Loss: 0.340879\n",
      "Epoch: 47 \tTraining Loss: 0.340448\n",
      "Epoch: 48 \tTraining Loss: 0.340476\n",
      "Epoch: 49 \tTraining Loss: 0.339852\n",
      "Epoch: 50 \tTraining Loss: 0.339661\n",
      "Epoch: 51 \tTraining Loss: 0.339417\n",
      "Epoch: 52 \tTraining Loss: 0.339487\n",
      "Epoch: 53 \tTraining Loss: 0.339348\n",
      "Epoch: 54 \tTraining Loss: 0.339213\n",
      "Epoch: 55 \tTraining Loss: 0.338816\n",
      "Epoch: 56 \tTraining Loss: 0.338608\n",
      "Epoch: 57 \tTraining Loss: 0.338484\n",
      "Epoch: 58 \tTraining Loss: 0.338145\n",
      "Epoch: 59 \tTraining Loss: 0.338312\n",
      "Epoch: 60 \tTraining Loss: 0.338228\n",
      "Epoch: 61 \tTraining Loss: 0.338145\n",
      "Epoch: 62 \tTraining Loss: 0.338002\n",
      "Epoch: 63 \tTraining Loss: 0.337655\n",
      "Epoch: 64 \tTraining Loss: 0.337465\n",
      "Epoch: 65 \tTraining Loss: 0.337401\n",
      "Epoch: 66 \tTraining Loss: 0.337313\n",
      "Epoch: 67 \tTraining Loss: 0.337176\n",
      "Epoch: 68 \tTraining Loss: 0.336931\n",
      "Epoch: 69 \tTraining Loss: 0.337008\n",
      "Epoch: 70 \tTraining Loss: 0.336867\n",
      "Epoch: 71 \tTraining Loss: 0.336738\n",
      "Epoch: 72 \tTraining Loss: 0.336551\n",
      "Epoch: 73 \tTraining Loss: 0.336294\n",
      "Epoch: 74 \tTraining Loss: 0.336301\n",
      "Epoch: 75 \tTraining Loss: 0.336234\n",
      "Epoch: 76 \tTraining Loss: 0.336076\n",
      "Epoch: 77 \tTraining Loss: 0.335973\n",
      "Epoch: 78 \tTraining Loss: 0.336057\n",
      "Epoch: 79 \tTraining Loss: 0.335925\n",
      "Epoch: 80 \tTraining Loss: 0.335895\n",
      "Epoch: 81 \tTraining Loss: 0.335748\n",
      "Epoch: 82 \tTraining Loss: 0.335596\n",
      "Epoch: 83 \tTraining Loss: 0.335469\n",
      "Epoch: 84 \tTraining Loss: 0.335375\n",
      "Epoch: 85 \tTraining Loss: 0.335317\n",
      "Epoch: 86 \tTraining Loss: 0.335196\n",
      "Epoch: 87 \tTraining Loss: 0.335115\n",
      "Epoch: 88 \tTraining Loss: 0.335123\n",
      "Epoch: 89 \tTraining Loss: 0.335200\n",
      "Epoch: 90 \tTraining Loss: 0.335055\n",
      "Epoch: 91 \tTraining Loss: 0.335018\n",
      "Epoch: 92 \tTraining Loss: 0.335020\n",
      "Epoch: 93 \tTraining Loss: 0.334922\n",
      "Epoch: 94 \tTraining Loss: 0.334827\n",
      "Epoch: 95 \tTraining Loss: 0.334791\n",
      "Epoch: 96 \tTraining Loss: 0.334716\n",
      "Epoch: 97 \tTraining Loss: 0.334688\n",
      "Epoch: 98 \tTraining Loss: 0.334628\n",
      "Epoch: 99 \tTraining Loss: 0.334589\n",
      "Epoch: 100 \tTraining Loss: 0.334550\n",
      "0.9413\n"
     ]
    }
   ],
   "source": [
    "X = df.loc[df[\"sentiment_score\"] != 3, \"tokenised_review_text\"]\n",
    "X = getCNNVectorEmbedding(X, custom_w2v)\n",
    "Y = df.loc[df[\"sentiment_score\"] != 3, \"sentiment_score\"]\n",
    "Y = torch.tensor(Y, dtype=torch.long)\n",
    "\n",
    "train, test = getCNNDataLoader(X, Y)\n",
    "\n",
    "classifier = train_cnn_model(train)\n",
    "\n",
    "actual, prediction = predict(classifier, test)\n",
    "\n",
    "accuracy = get_accuracy(actual, prediction)\n",
    "\n",
    "add_report_entry(\"CNN\", \"Binary\", \"50x300\", \"Custom\", accuracy)\n",
    "\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNN Ternary Classification (Pretrained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.984988\n",
      "Epoch: 2 \tTraining Loss: 0.830215\n",
      "Epoch: 3 \tTraining Loss: 0.814236\n",
      "Epoch: 4 \tTraining Loss: 0.780836\n",
      "Epoch: 5 \tTraining Loss: 0.748311\n",
      "Epoch: 6 \tTraining Loss: 0.737056\n",
      "Epoch: 7 \tTraining Loss: 0.730275\n",
      "Epoch: 8 \tTraining Loss: 0.725284\n",
      "Epoch: 9 \tTraining Loss: 0.721329\n",
      "Epoch: 10 \tTraining Loss: 0.718084\n",
      "Epoch: 11 \tTraining Loss: 0.715435\n",
      "Epoch: 12 \tTraining Loss: 0.713132\n",
      "Epoch: 13 \tTraining Loss: 0.711093\n",
      "Epoch: 14 \tTraining Loss: 0.709252\n",
      "Epoch: 15 \tTraining Loss: 0.707556\n",
      "Epoch: 16 \tTraining Loss: 0.705957\n",
      "Epoch: 17 \tTraining Loss: 0.704509\n",
      "Epoch: 18 \tTraining Loss: 0.703131\n",
      "Epoch: 19 \tTraining Loss: 0.701749\n",
      "Epoch: 20 \tTraining Loss: 0.700552\n",
      "Epoch: 21 \tTraining Loss: 0.699552\n",
      "Epoch: 22 \tTraining Loss: 0.698885\n",
      "Epoch: 23 \tTraining Loss: 0.698664\n",
      "Epoch: 24 \tTraining Loss: 0.698677\n",
      "Epoch: 25 \tTraining Loss: 0.698108\n",
      "Epoch: 26 \tTraining Loss: 0.696847\n",
      "Epoch: 27 \tTraining Loss: 0.695214\n",
      "Epoch: 28 \tTraining Loss: 0.693951\n",
      "Epoch: 29 \tTraining Loss: 0.692594\n",
      "Epoch: 30 \tTraining Loss: 0.691463\n",
      "Epoch: 31 \tTraining Loss: 0.690465\n",
      "Epoch: 32 \tTraining Loss: 0.689766\n",
      "Epoch: 33 \tTraining Loss: 0.689532\n",
      "Epoch: 34 \tTraining Loss: 0.689472\n",
      "Epoch: 35 \tTraining Loss: 0.689437\n",
      "Epoch: 36 \tTraining Loss: 0.688210\n",
      "Epoch: 37 \tTraining Loss: 0.687014\n",
      "Epoch: 38 \tTraining Loss: 0.686237\n",
      "Epoch: 39 \tTraining Loss: 0.685156\n",
      "Epoch: 40 \tTraining Loss: 0.684475\n",
      "Epoch: 41 \tTraining Loss: 0.683748\n",
      "Epoch: 42 \tTraining Loss: 0.682981\n",
      "Epoch: 43 \tTraining Loss: 0.682183\n",
      "Epoch: 44 \tTraining Loss: 0.681610\n",
      "Epoch: 45 \tTraining Loss: 0.680756\n",
      "Epoch: 46 \tTraining Loss: 0.680400\n",
      "Epoch: 47 \tTraining Loss: 0.680221\n",
      "Epoch: 48 \tTraining Loss: 0.680025\n",
      "Epoch: 49 \tTraining Loss: 0.679683\n",
      "Epoch: 50 \tTraining Loss: 0.678912\n",
      "Epoch: 51 \tTraining Loss: 0.677700\n",
      "Epoch: 52 \tTraining Loss: 0.677086\n",
      "Epoch: 53 \tTraining Loss: 0.676083\n",
      "Epoch: 54 \tTraining Loss: 0.674648\n",
      "Epoch: 55 \tTraining Loss: 0.673489\n",
      "Epoch: 56 \tTraining Loss: 0.672729\n",
      "Epoch: 57 \tTraining Loss: 0.672220\n",
      "Epoch: 58 \tTraining Loss: 0.671673\n",
      "Epoch: 59 \tTraining Loss: 0.671313\n",
      "Epoch: 60 \tTraining Loss: 0.670994\n",
      "Epoch: 61 \tTraining Loss: 0.670745\n",
      "Epoch: 62 \tTraining Loss: 0.670458\n",
      "Epoch: 63 \tTraining Loss: 0.669597\n",
      "Epoch: 64 \tTraining Loss: 0.668995\n",
      "Epoch: 65 \tTraining Loss: 0.668239\n",
      "Epoch: 66 \tTraining Loss: 0.667598\n",
      "Epoch: 67 \tTraining Loss: 0.666707\n",
      "Epoch: 68 \tTraining Loss: 0.666564\n",
      "Epoch: 69 \tTraining Loss: 0.666086\n",
      "Epoch: 70 \tTraining Loss: 0.665882\n",
      "Epoch: 71 \tTraining Loss: 0.665468\n",
      "Epoch: 72 \tTraining Loss: 0.665703\n",
      "Epoch: 73 \tTraining Loss: 0.664636\n",
      "Epoch: 74 \tTraining Loss: 0.664736\n",
      "Epoch: 75 \tTraining Loss: 0.664285\n",
      "Epoch: 76 \tTraining Loss: 0.663872\n",
      "Epoch: 77 \tTraining Loss: 0.663218\n",
      "Epoch: 78 \tTraining Loss: 0.663004\n",
      "Epoch: 79 \tTraining Loss: 0.662181\n",
      "Epoch: 80 \tTraining Loss: 0.661638\n",
      "Epoch: 81 \tTraining Loss: 0.661482\n",
      "Epoch: 82 \tTraining Loss: 0.661638\n",
      "Epoch: 83 \tTraining Loss: 0.660814\n",
      "Epoch: 84 \tTraining Loss: 0.660167\n",
      "Epoch: 85 \tTraining Loss: 0.659729\n",
      "Epoch: 86 \tTraining Loss: 0.659124\n",
      "Epoch: 87 \tTraining Loss: 0.658763\n",
      "Epoch: 88 \tTraining Loss: 0.658643\n",
      "Epoch: 89 \tTraining Loss: 0.658436\n",
      "Epoch: 90 \tTraining Loss: 0.658544\n",
      "Epoch: 91 \tTraining Loss: 0.658543\n",
      "Epoch: 92 \tTraining Loss: 0.658458\n",
      "Epoch: 93 \tTraining Loss: 0.658283\n",
      "Epoch: 94 \tTraining Loss: 0.657909\n",
      "Epoch: 95 \tTraining Loss: 0.657049\n",
      "Epoch: 96 \tTraining Loss: 0.656656\n",
      "Epoch: 97 \tTraining Loss: 0.656398\n",
      "Epoch: 98 \tTraining Loss: 0.655274\n",
      "Epoch: 99 \tTraining Loss: 0.654511\n",
      "Epoch: 100 \tTraining Loss: 0.654589\n",
      "0.82004\n"
     ]
    }
   ],
   "source": [
    "X = getCNNVectorEmbedding(df[\"tokenised_review_text\"], google_w2v)\n",
    "Y = torch.tensor(df[\"sentiment_score\"], dtype=torch.long)\n",
    "\n",
    "train, test = getCNNDataLoader(X, Y)\n",
    "\n",
    "classifier = train_cnn_model(train, output_features=3)\n",
    "\n",
    "actual, prediction = predict(classifier, test)\n",
    "\n",
    "accuracy = get_accuracy(actual, prediction)\n",
    "\n",
    "add_report_entry(\"CNN\", \"Ternary\", \"50x300\", \"Pretrained\", accuracy)\n",
    "\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNN Ternary Classification (Custom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.790708\n",
      "Epoch: 2 \tTraining Loss: 0.723843\n",
      "Epoch: 3 \tTraining Loss: 0.714784\n",
      "Epoch: 4 \tTraining Loss: 0.709414\n",
      "Epoch: 5 \tTraining Loss: 0.705198\n",
      "Epoch: 6 \tTraining Loss: 0.701657\n",
      "Epoch: 7 \tTraining Loss: 0.698935\n",
      "Epoch: 8 \tTraining Loss: 0.696338\n",
      "Epoch: 9 \tTraining Loss: 0.694440\n",
      "Epoch: 10 \tTraining Loss: 0.692637\n",
      "Epoch: 11 \tTraining Loss: 0.691433\n",
      "Epoch: 12 \tTraining Loss: 0.689899\n",
      "Epoch: 13 \tTraining Loss: 0.689247\n",
      "Epoch: 14 \tTraining Loss: 0.687790\n",
      "Epoch: 15 \tTraining Loss: 0.686532\n",
      "Epoch: 16 \tTraining Loss: 0.685216\n",
      "Epoch: 17 \tTraining Loss: 0.683821\n",
      "Epoch: 18 \tTraining Loss: 0.682526\n",
      "Epoch: 19 \tTraining Loss: 0.681352\n",
      "Epoch: 20 \tTraining Loss: 0.680586\n",
      "Epoch: 21 \tTraining Loss: 0.679815\n",
      "Epoch: 22 \tTraining Loss: 0.678739\n",
      "Epoch: 23 \tTraining Loss: 0.679227\n",
      "Epoch: 24 \tTraining Loss: 0.678325\n",
      "Epoch: 25 \tTraining Loss: 0.677058\n",
      "Epoch: 26 \tTraining Loss: 0.675638\n",
      "Epoch: 27 \tTraining Loss: 0.674569\n",
      "Epoch: 28 \tTraining Loss: 0.674393\n",
      "Epoch: 29 \tTraining Loss: 0.674623\n",
      "Epoch: 30 \tTraining Loss: 0.673893\n",
      "Epoch: 31 \tTraining Loss: 0.673250\n",
      "Epoch: 32 \tTraining Loss: 0.672336\n",
      "Epoch: 33 \tTraining Loss: 0.672093\n",
      "Epoch: 34 \tTraining Loss: 0.670815\n",
      "Epoch: 35 \tTraining Loss: 0.670796\n",
      "Epoch: 36 \tTraining Loss: 0.670898\n",
      "Epoch: 37 \tTraining Loss: 0.669146\n",
      "Epoch: 38 \tTraining Loss: 0.669227\n",
      "Epoch: 39 \tTraining Loss: 0.668462\n",
      "Epoch: 40 \tTraining Loss: 0.668216\n",
      "Epoch: 41 \tTraining Loss: 0.667760\n",
      "Epoch: 42 \tTraining Loss: 0.667414\n",
      "Epoch: 43 \tTraining Loss: 0.666955\n",
      "Epoch: 44 \tTraining Loss: 0.667372\n",
      "Epoch: 45 \tTraining Loss: 0.666793\n",
      "Epoch: 46 \tTraining Loss: 0.666469\n",
      "Epoch: 47 \tTraining Loss: 0.666011\n",
      "Epoch: 48 \tTraining Loss: 0.665543\n",
      "Epoch: 49 \tTraining Loss: 0.664335\n",
      "Epoch: 50 \tTraining Loss: 0.664467\n",
      "Epoch: 51 \tTraining Loss: 0.663617\n",
      "Epoch: 52 \tTraining Loss: 0.663827\n",
      "Epoch: 53 \tTraining Loss: 0.664272\n",
      "Epoch: 54 \tTraining Loss: 0.663350\n",
      "Epoch: 55 \tTraining Loss: 0.662582\n",
      "Epoch: 56 \tTraining Loss: 0.662686\n",
      "Epoch: 57 \tTraining Loss: 0.662550\n",
      "Epoch: 58 \tTraining Loss: 0.662458\n",
      "Epoch: 59 \tTraining Loss: 0.661915\n",
      "Epoch: 60 \tTraining Loss: 0.662414\n",
      "Epoch: 61 \tTraining Loss: 0.662260\n",
      "Epoch: 62 \tTraining Loss: 0.662013\n",
      "Epoch: 63 \tTraining Loss: 0.661449\n",
      "Epoch: 64 \tTraining Loss: 0.662134\n",
      "Epoch: 65 \tTraining Loss: 0.661274\n",
      "Epoch: 66 \tTraining Loss: 0.661987\n",
      "Epoch: 67 \tTraining Loss: 0.660392\n",
      "Epoch: 68 \tTraining Loss: 0.659712\n",
      "Epoch: 69 \tTraining Loss: 0.660631\n",
      "Epoch: 70 \tTraining Loss: 0.659711\n",
      "Epoch: 71 \tTraining Loss: 0.659827\n",
      "Epoch: 72 \tTraining Loss: 0.659926\n",
      "Epoch: 73 \tTraining Loss: 0.659706\n",
      "Epoch: 74 \tTraining Loss: 0.659418\n",
      "Epoch: 75 \tTraining Loss: 0.659746\n",
      "Epoch: 76 \tTraining Loss: 0.659287\n",
      "Epoch: 77 \tTraining Loss: 0.659219\n",
      "Epoch: 78 \tTraining Loss: 0.659362\n",
      "Epoch: 79 \tTraining Loss: 0.658831\n",
      "Epoch: 80 \tTraining Loss: 0.658179\n",
      "Epoch: 81 \tTraining Loss: 0.658465\n",
      "Epoch: 82 \tTraining Loss: 0.659175\n",
      "Epoch: 83 \tTraining Loss: 0.659090\n",
      "Epoch: 84 \tTraining Loss: 0.657609\n",
      "Epoch: 85 \tTraining Loss: 0.657696\n",
      "Epoch: 86 \tTraining Loss: 0.658508\n",
      "Epoch: 87 \tTraining Loss: 0.657271\n",
      "Epoch: 88 \tTraining Loss: 0.656693\n",
      "Epoch: 89 \tTraining Loss: 0.657979\n",
      "Epoch: 90 \tTraining Loss: 0.657263\n",
      "Epoch: 91 \tTraining Loss: 0.657867\n",
      "Epoch: 92 \tTraining Loss: 0.657946\n",
      "Epoch: 93 \tTraining Loss: 0.657448\n",
      "Epoch: 94 \tTraining Loss: 0.657790\n",
      "Epoch: 95 \tTraining Loss: 0.657259\n",
      "Epoch: 96 \tTraining Loss: 0.657341\n",
      "Epoch: 97 \tTraining Loss: 0.656820\n",
      "Epoch: 98 \tTraining Loss: 0.656943\n",
      "Epoch: 99 \tTraining Loss: 0.657204\n",
      "Epoch: 100 \tTraining Loss: 0.656834\n",
      "0.82984\n"
     ]
    }
   ],
   "source": [
    "X = getCNNVectorEmbedding(df[\"tokenised_review_text\"], custom_w2v)\n",
    "Y = torch.tensor(df[\"sentiment_score\"], dtype=torch.long)\n",
    "\n",
    "train, test = getCNNDataLoader(X, Y)\n",
    "\n",
    "classifier = train_cnn_model(train, output_features=3)\n",
    "\n",
    "actual, prediction = predict(classifier, test)\n",
    "\n",
    "accuracy = get_accuracy(actual, prediction)\n",
    "\n",
    "add_report_entry(\"CNN\", \"Ternary\", \"50x300\", \"Custom\", accuracy)\n",
    "\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>classification_type</th>\n",
       "      <th>vector_type</th>\n",
       "      <th>w2v_model_type</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Perceptron</td>\n",
       "      <td>Binary</td>\n",
       "      <td>Mean</td>\n",
       "      <td>Pretrained</td>\n",
       "      <td>0.733300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Perceptron</td>\n",
       "      <td>Binary</td>\n",
       "      <td>Mean</td>\n",
       "      <td>Custom</td>\n",
       "      <td>0.886100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SVM</td>\n",
       "      <td>Binary</td>\n",
       "      <td>Mean</td>\n",
       "      <td>Pretrained</td>\n",
       "      <td>0.884450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SVM</td>\n",
       "      <td>Binary</td>\n",
       "      <td>Mean</td>\n",
       "      <td>Custom</td>\n",
       "      <td>0.910525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FFNN</td>\n",
       "      <td>Binary</td>\n",
       "      <td>Mean</td>\n",
       "      <td>Pretrained</td>\n",
       "      <td>0.911825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>FFNN</td>\n",
       "      <td>Ternary</td>\n",
       "      <td>Mean</td>\n",
       "      <td>Pretrained</td>\n",
       "      <td>0.785200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>FFNN</td>\n",
       "      <td>Binary</td>\n",
       "      <td>Concat</td>\n",
       "      <td>Pretrained</td>\n",
       "      <td>0.887750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>FFNN</td>\n",
       "      <td>Ternary</td>\n",
       "      <td>Concat</td>\n",
       "      <td>Pretrained</td>\n",
       "      <td>0.736100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>FFNN</td>\n",
       "      <td>Binary</td>\n",
       "      <td>Mean</td>\n",
       "      <td>Custom</td>\n",
       "      <td>0.922875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>FFNN</td>\n",
       "      <td>Ternary</td>\n",
       "      <td>Mean</td>\n",
       "      <td>Custom</td>\n",
       "      <td>0.803840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>FFNN</td>\n",
       "      <td>Binary</td>\n",
       "      <td>Concat</td>\n",
       "      <td>Custom</td>\n",
       "      <td>0.892375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>FFNN</td>\n",
       "      <td>Ternary</td>\n",
       "      <td>Concat</td>\n",
       "      <td>Custom</td>\n",
       "      <td>0.750660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>CNN</td>\n",
       "      <td>Binary</td>\n",
       "      <td>50x300</td>\n",
       "      <td>Pretrained</td>\n",
       "      <td>0.931450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>CNN</td>\n",
       "      <td>Binary</td>\n",
       "      <td>50x300</td>\n",
       "      <td>Custom</td>\n",
       "      <td>0.941300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>CNN</td>\n",
       "      <td>Ternary</td>\n",
       "      <td>50x300</td>\n",
       "      <td>Pretrained</td>\n",
       "      <td>0.820040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>CNN</td>\n",
       "      <td>Ternary</td>\n",
       "      <td>50x300</td>\n",
       "      <td>Custom</td>\n",
       "      <td>0.829840</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         model classification_type vector_type w2v_model_type  accuracy\n",
       "0   Perceptron              Binary        Mean     Pretrained  0.733300\n",
       "1   Perceptron              Binary        Mean         Custom  0.886100\n",
       "2          SVM              Binary        Mean     Pretrained  0.884450\n",
       "3          SVM              Binary        Mean         Custom  0.910525\n",
       "4         FFNN              Binary        Mean     Pretrained  0.911825\n",
       "5         FFNN             Ternary        Mean     Pretrained  0.785200\n",
       "6         FFNN              Binary      Concat     Pretrained  0.887750\n",
       "7         FFNN             Ternary      Concat     Pretrained  0.736100\n",
       "8         FFNN              Binary        Mean         Custom  0.922875\n",
       "9         FFNN             Ternary        Mean         Custom  0.803840\n",
       "10        FFNN              Binary      Concat         Custom  0.892375\n",
       "11        FFNN             Ternary      Concat         Custom  0.750660\n",
       "12         CNN              Binary      50x300     Pretrained  0.931450\n",
       "13         CNN              Binary      50x300         Custom  0.941300\n",
       "14         CNN             Ternary      50x300     Pretrained  0.820040\n",
       "15         CNN             Ternary      50x300         Custom  0.829840"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_report"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
