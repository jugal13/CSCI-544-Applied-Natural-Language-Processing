{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import Counter\n",
    "from torch.nn.utils.rnn import pad_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = \"data/train\"\n",
    "dev_file = \"data/dev\"\n",
    "test_file = \"data/test\"\n",
    "\n",
    "padding_value = 9\n",
    "unknown_value = 21012\n",
    "threshold_value = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapSentenceToList(sentences, entity):\n",
    "  sentences = sentences.split(\"\\n\")\n",
    "\n",
    "  if entity:\n",
    "    sentences = list(map(lambda x: tuple(x.split(\" \")[1:]), sentences))\n",
    "  else:\n",
    "    sentences = list(map(lambda x: x.split(\" \")[1], sentences))\n",
    "\n",
    "  return sentences\n",
    "\n",
    "\n",
    "def readFile(file_path, entity=True):\n",
    "  with open(file_path, \"r\") as fin:\n",
    "    sentences = \"\".join(list(fin)).split(\"\\n\\n\")\n",
    "    sentences[-1] = sentences[-1].rstrip()\n",
    "\n",
    "    sentences = list(\n",
    "      map(\n",
    "        lambda x: mapSentenceToList(x, entity),\n",
    "        sentences\n",
    "      )\n",
    "    )\n",
    "\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def writeFile(file_path, sentences):\n",
    "  with open(file_path, \"w\") as fout:\n",
    "    for sentence in sentences:\n",
    "      for index, (word, tag) in enumerate(sentence):\n",
    "        s = str(index + 1) + \" \" + word + \" \" + tag + \"\\n\"\n",
    "        fout.write(s)\n",
    "      fout.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLSTM(nn.Module):\n",
    "  def __init__(self, vocab_size, num_classes):\n",
    "    super(CustomLSTM, self).__init__()\n",
    "    self.embedding_dim = 100\n",
    "    self.hidden_dim = 256\n",
    "    self.lstm_dropout = 0.33\n",
    "    self.linear_dim = 128\n",
    "\n",
    "    self.embedding_layer = nn.Embedding(\n",
    "      num_embeddings=vocab_size,\n",
    "      embedding_dim=100,\n",
    "      padding_idx=0\n",
    "    )\n",
    "\n",
    "    self.lstm_layer = nn.LSTM(\n",
    "      input_size=self.embedding_dim,\n",
    "      hidden_size=self.hidden_dim,\n",
    "      bidirectional=True,\n",
    "      batch_first=True\n",
    "    )\n",
    "\n",
    "    self.dropout_layer = nn.Dropout(self.lstm_dropout)\n",
    "\n",
    "    self.linear_layer = nn.Linear(\n",
    "      in_features=self.hidden_dim * 2,\n",
    "      out_features=self.linear_dim\n",
    "    )\n",
    "\n",
    "    self.elu_layer = nn.ELU()\n",
    "\n",
    "    self.classifier = nn.Linear(\n",
    "      in_features=self.linear_dim,\n",
    "      out_features=num_classes,\n",
    "    )\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.embedding_layer(x)\n",
    "    x, _ = self.lstm_layer(x)\n",
    "    x = self.dropout_layer(x)\n",
    "    x = self.linear_layer(x)\n",
    "    x = self.elu_layer(x)\n",
    "    x = self.classifier(x)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_lstm_model(dataloader, vocab_size, num_classes, weight_balance):\n",
    "  model = CustomLSTM(vocab_size, num_classes)\n",
    "\n",
    "  learning_rate = 0.095\n",
    "  criterion = nn.CrossEntropyLoss(ignore_index=padding_value, weight=weight_balance)\n",
    "  optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "  exp_scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.95)\n",
    "  no_epochs = 40\n",
    "\n",
    "  for epoch in range(no_epochs):\n",
    "    train_loss = 0.0\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    match = 0\n",
    "    word_count = 0\n",
    "\n",
    "    for data, target in dataloader:\n",
    "      optimizer.zero_grad()\n",
    "\n",
    "      output = model(data)\n",
    "\n",
    "      output = output.view(-1, num_classes)\n",
    "      target = target.view(-1)\n",
    "\n",
    "      for i in range(output.shape[0]):\n",
    "        pred = torch.argmax(output[i])\n",
    "        actual = target[i].item()\n",
    "        word_count += 1\n",
    "        if (actual == padding_value):\n",
    "          continue\n",
    "        if (pred == actual):\n",
    "          match += 1\n",
    "\n",
    "      loss = criterion(output, target)\n",
    "\n",
    "      loss.backward()\n",
    "\n",
    "      optimizer.step()\n",
    "\n",
    "      train_loss += loss.item()\n",
    "\n",
    "    train_loss = train_loss/len(dataloader.dataset)\n",
    "\n",
    "    exp_scheduler.step()\n",
    "    \n",
    "    accuracy = (match / word_count) * 100\n",
    "\n",
    "    print(\n",
    "      'Epoch: {} \\tTraining Loss: {:.6f}'.format(\n",
    "        epoch+1,\n",
    "        train_loss,\n",
    "      )\n",
    "    )\n",
    "    print(\"Accuracy: \", accuracy)\n",
    "\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "  def __init__(self, X, Y):\n",
    "    self.X = X\n",
    "    self.Y = Y\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.X)\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "    return self.X[index], self.Y[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate_fn(batch):\n",
    "    sentences = [torch.LongTensor(item[0]) for item in batch]\n",
    "    tags = [torch.LongTensor(item[1]) for item in batch]\n",
    "\n",
    "    padded_sentences = pad_sequence(\n",
    "        sentences, batch_first=True)\n",
    "    padded_tags = pad_sequence(\n",
    "        tags, batch_first=True, padding_value=padding_value)\n",
    "\n",
    "    return padded_sentences, padded_tags\n",
    "\n",
    "\n",
    "def getDatasetAndDataLoader(X, Y, batch_size=32):\n",
    "    dataset = CustomDataset(X, Y)\n",
    "\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size,\n",
    "                            shuffle=False, collate_fn=custom_collate_fn)\n",
    "\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21010\n",
      "21012\n",
      "{'I-ORG': 0, 'B-PER': 1, 'B-MISC': 2, 'B-LOC': 3, 'B-ORG': 4, 'I-LOC': 5, 'I-PER': 6, 'I-MISC': 7, 'O': 8}\n"
     ]
    }
   ],
   "source": [
    "train_sentences = readFile(train_file)\n",
    "\n",
    "train_sentences = sorted(train_sentences, key=lambda x: len(x))\n",
    "\n",
    "words_list = []\n",
    "nerTagSet = set()\n",
    "nerTagList = []\n",
    "nerTagFreq = {}\n",
    "for sentence in train_sentences:\n",
    "  for word, nerTag in sentence:\n",
    "    words_list.append(word.lower())\n",
    "    nerTagSet.add(nerTag)\n",
    "    nerTagList.append(nerTag)\n",
    "    if nerTag in nerTagFreq:\n",
    "      nerTagFreq[nerTag] += 1\n",
    "    else:\n",
    "      nerTagFreq[nerTag] = 1\n",
    "  \n",
    "words_counter = Counter(words_list)\n",
    "words_counter = [word for word, count in words_counter.items() if count >= threshold_value]\n",
    "vocab = {word: index + 1 for index, word in enumerate(words_counter)}\n",
    "vocab[\"<PAD>\"] = 0\n",
    "vocab[\"<UNK>\"] = unknown_value\n",
    "\n",
    "tag_map = { tag: index for index, tag in enumerate(list(nerTagSet)) }\n",
    "\n",
    "map_index_to_tag = {\n",
    "  index: tag for tag, index in tag_map.items()\n",
    "}\n",
    "\n",
    "print(len(words_counter))\n",
    "print(len(vocab))\n",
    "print(tag_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "Y = []\n",
    "\n",
    "for sentence in train_sentences:\n",
    "  ip = []\n",
    "  op = []\n",
    "\n",
    "  for word, tag in sentence:\n",
    "    ip.append(vocab.get(word.lower(), unknown_value))\n",
    "    op.append(tag_map[tag])\n",
    "\n",
    "  X.append(ip)\n",
    "  Y.append(op)\n",
    "\n",
    "train_dataloader = getDatasetAndDataLoader(X, Y, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.4496, 0.2523, 0.4844, 0.2332, 0.2634, 1.4393, 0.3678, 1.4418, 0.0098])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = [0 for _ in range(len(tag_map))]\n",
    "\n",
    "for index, tag in enumerate(tag_map):\n",
    "  weights[index] = len(train_sentences) / (len(tag_map) * nerTagFreq[tag])\n",
    "\n",
    "weight_balance = torch.Tensor(weights)\n",
    "\n",
    "weight_balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.176754\n",
      "Accuracy:  51.28986122703998\n",
      "Epoch: 2 \tTraining Loss: 0.122137\n",
      "Accuracy:  60.96735931116285\n",
      "Epoch: 3 \tTraining Loss: 0.088448\n",
      "Accuracy:  67.6092294024904\n",
      "Epoch: 4 \tTraining Loss: 0.065097\n",
      "Accuracy:  72.49532623604583\n",
      "Epoch: 5 \tTraining Loss: 0.049974\n",
      "Accuracy:  76.08985341709499\n",
      "Epoch: 6 \tTraining Loss: 0.038772\n",
      "Accuracy:  78.89801676209444\n",
      "Epoch: 7 \tTraining Loss: 0.031037\n",
      "Accuracy:  81.29274114425456\n",
      "Epoch: 8 \tTraining Loss: 0.026076\n",
      "Accuracy:  83.02313208081341\n",
      "Epoch: 9 \tTraining Loss: 0.022166\n",
      "Accuracy:  84.45918571561062\n",
      "Epoch: 10 \tTraining Loss: 0.018915\n",
      "Accuracy:  85.82055675145338\n",
      "Epoch: 11 \tTraining Loss: 0.016891\n",
      "Accuracy:  86.91980650861291\n",
      "Epoch: 12 \tTraining Loss: 0.015038\n",
      "Accuracy:  87.77450736331376\n",
      "Epoch: 13 \tTraining Loss: 0.014151\n",
      "Accuracy:  88.19575627114176\n",
      "Epoch: 14 \tTraining Loss: 0.012490\n",
      "Accuracy:  89.03142038493267\n",
      "Epoch: 15 \tTraining Loss: 0.011243\n",
      "Accuracy:  89.82705853065647\n",
      "Epoch: 16 \tTraining Loss: 0.010675\n",
      "Accuracy:  90.12383644022708\n",
      "Epoch: 17 \tTraining Loss: 0.009626\n",
      "Accuracy:  90.5982905982906\n",
      "Epoch: 18 \tTraining Loss: 0.008846\n",
      "Accuracy:  91.07713785041027\n",
      "Epoch: 19 \tTraining Loss: 0.008665\n",
      "Accuracy:  91.40076244587952\n",
      "Epoch: 20 \tTraining Loss: 0.008012\n",
      "Accuracy:  91.8010221265504\n",
      "Epoch: 21 \tTraining Loss: 0.007649\n",
      "Accuracy:  91.96600721443669\n",
      "Epoch: 22 \tTraining Loss: 0.007535\n",
      "Accuracy:  92.11878926327812\n",
      "Epoch: 23 \tTraining Loss: 0.007338\n",
      "Accuracy:  92.42142463159026\n",
      "Epoch: 24 \tTraining Loss: 0.006564\n",
      "Accuracy:  92.81777933976677\n",
      "Epoch: 25 \tTraining Loss: 0.006471\n",
      "Accuracy:  92.82461304163189\n",
      "Epoch: 26 \tTraining Loss: 0.006058\n",
      "Accuracy:  93.04866083849522\n",
      "Epoch: 27 \tTraining Loss: 0.006010\n",
      "Accuracy:  93.18533487579747\n",
      "Epoch: 28 \tTraining Loss: 0.005753\n",
      "Accuracy:  93.42939565669435\n",
      "Epoch: 29 \tTraining Loss: 0.005471\n",
      "Accuracy:  93.59291637989526\n",
      "Epoch: 30 \tTraining Loss: 0.005449\n",
      "Accuracy:  93.63196610483875\n",
      "Epoch: 31 \tTraining Loss: 0.005280\n",
      "Accuracy:  93.83648903923033\n",
      "Epoch: 32 \tTraining Loss: 0.005430\n",
      "Accuracy:  93.88871804634226\n",
      "Epoch: 33 \tTraining Loss: 0.005009\n",
      "Accuracy:  94.064441808588\n",
      "Epoch: 34 \tTraining Loss: 0.004877\n",
      "Accuracy:  94.11081335695842\n",
      "Epoch: 35 \tTraining Loss: 0.004697\n",
      "Accuracy:  94.237236841463\n",
      "Epoch: 36 \tTraining Loss: 0.004773\n",
      "Accuracy:  94.26261916267627\n",
      "Epoch: 37 \tTraining Loss: 0.004477\n",
      "Accuracy:  94.4163774546413\n",
      "Epoch: 38 \tTraining Loss: 0.004439\n",
      "Accuracy:  94.46714209706786\n",
      "Epoch: 39 \tTraining Loss: 0.004455\n",
      "Accuracy:  94.57989817784221\n",
      "Epoch: 40 \tTraining Loss: 0.004238\n",
      "Accuracy:  94.64140149462821\n"
     ]
    }
   ],
   "source": [
    "model = train_lstm_model(train_dataloader, len(vocab) + 1, len(tag_map), weight_balance=torch.Tensor(weight_balance))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, \"blstm1.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_dev(pred_model, dataloader):\n",
    "  # pred_model.eval()\n",
    "  y_actual = []\n",
    "  y_pred = []\n",
    "  \n",
    "  # with torch.no_grad():\n",
    "  for data, target in dataloader:\n",
    "    pred = pred_model(data)\n",
    "\n",
    "    pred = torch.argmax(pred, 2)\n",
    "\n",
    "    y_pred.extend(pred)\n",
    "    y_actual.extend(target)\n",
    "\n",
    "  return y_pred, y_actual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_sentences = readFile(dev_file)\n",
    "\n",
    "X_dev = []\n",
    "Y_dev = []\n",
    "\n",
    "for sentence in dev_sentences:\n",
    "  ip_dev = []\n",
    "  op_dev = []\n",
    "\n",
    "  for word, tag in sentence:\n",
    "    ip_dev.append(vocab.get(word.lower(), unknown_value))\n",
    "    op_dev.append(tag_map[tag])\n",
    "\n",
    "  X_dev.append(ip_dev)\n",
    "  Y_dev.append(op_dev)\n",
    "\n",
    "dev_dataloader = getDatasetAndDataLoader(X_dev, Y_dev, batch_size=2)\n",
    "\n",
    "y_pred, y_actual = predict_dev(model, dev_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_sentences = []\n",
    "for index, sentence in enumerate(dev_sentences):\n",
    "  preds = y_pred[index].tolist()\n",
    "  if not isinstance(preds, list):\n",
    "    preds = [preds]\n",
    "  curr_sentence = []\n",
    "  for word_index, (word, _) in enumerate(sentence):\n",
    "    curr_sentence.append((word, map_index_to_tag[preds[word_index]]))\n",
    "  pred_sentences.append(curr_sentence)\n",
    "\n",
    "writeFile(\"dev1.out\", pred_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestDataset(Dataset):\n",
    "    def __init__(self, X):\n",
    "        self.X = X\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.X[index]\n",
    "\n",
    "\n",
    "def test_collate_fn(batch):\n",
    "    sentences = [torch.LongTensor(item) for item in batch]\n",
    "\n",
    "    padded_sentences = pad_sequence(\n",
    "        sentences, batch_first=True)\n",
    "\n",
    "    return padded_sentences\n",
    "\n",
    "\n",
    "def getTestDataloader(X, batch_size=2):\n",
    "    dataset = TestDataset(X)\n",
    "\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=False, collate_fn=test_collate_fn)\n",
    "\n",
    "\n",
    "def predict_test(pred_model, dataloader):\n",
    "    # pred_model.eval()\n",
    "    y_pred = []\n",
    "\n",
    "    # with torch.no_grad():\n",
    "    for data in dataloader:\n",
    "        pred = pred_model(data)\n",
    "\n",
    "        pred = torch.argmax(pred, 2)\n",
    "\n",
    "        y_pred.extend(pred)\n",
    "\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentences = readFile(test_file, entity=False)\n",
    "\n",
    "X_test = []\n",
    "\n",
    "for sentence in test_sentences:\n",
    "  ip_test = []\n",
    "  for word in sentence:\n",
    "    ip_test.append(vocab.get(word.lower(), unknown_value))\n",
    "  X_test.append(ip_test)\n",
    "\n",
    "test_dataloader = getTestDataloader(X_test)\n",
    "\n",
    "y_pred = predict_test(model, test_dataloader)\n",
    "\n",
    "pred_sentences = []\n",
    "for index, sentence in enumerate(test_sentences):\n",
    "  preds = y_pred[index].tolist()\n",
    "  if not isinstance(preds, list):\n",
    "    preds = [preds]\n",
    "  curr_sentence = []\n",
    "  for word_index, word in enumerate(sentence):\n",
    "    curr_sentence.append((word, map_index_to_tag[preds[word_index]]))\n",
    "  pred_sentences.append(curr_sentence)\n",
    "\n",
    "writeFile(\"test1.out\", pred_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "processed 51578 tokens with 5942 phrases; found: 9516 phrases; correct: 4264.\n",
    "- accuracy:  87.89%; precision:  44.81%; recall:  71.76%; FB1:  55.17\n",
    "- LOC: precision:  69.04%; recall:  76.59%; FB1:  72.62  2038\n",
    "- MISC: precision:  47.49%; recall:  68.66%; FB1:  56.14  1333\n",
    "- ORG: precision:  32.21%; recall:  63.61%; FB1:  42.77  2648\n",
    "- PER: precision:  39.21%; recall:  74.43%; FB1:  51.36  3497"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
