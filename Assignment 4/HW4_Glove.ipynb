{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = \"data/train\"\n",
    "dev_file = \"data/dev\"\n",
    "test_file = \"data/test\"\n",
    "glove_file = \"glove.6B.100d\"\n",
    "\n",
    "padding_value = 9\n",
    "unknown_value = 756774\n",
    "threshold_value = 0\n",
    "embedding_dim = 101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapSentenceToList(sentences, entity):\n",
    "  sentences = sentences.split(\"\\n\")\n",
    "\n",
    "  if entity:\n",
    "    sentences = list(map(lambda x: tuple(x.split(\" \")[1:]), sentences))\n",
    "  else:\n",
    "    sentences = list(map(lambda x: x.split(\" \")[1], sentences))\n",
    "\n",
    "  return sentences\n",
    "\n",
    "\n",
    "def readFile(file_path, entity=True):\n",
    "  with open(file_path, \"r\") as fin:\n",
    "    sentences = \"\".join(list(fin)).split(\"\\n\\n\")\n",
    "    sentences[-1] = sentences[-1].rstrip()\n",
    "\n",
    "    sentences = list(\n",
    "      map(\n",
    "        lambda x: mapSentenceToList(x, entity),\n",
    "        sentences\n",
    "      )\n",
    "    )\n",
    "\n",
    "    return sentences\n",
    "\n",
    "def writeFile(file_path, sentences):\n",
    "  with open(file_path, \"w\") as fout:\n",
    "    for sentence in sentences:\n",
    "      for index, (word, tag) in enumerate(sentence):\n",
    "        s = str(index + 1) + \" \" + word + \" \" + tag + \"\\n\"\n",
    "        fout.write(s)\n",
    "      fout.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readGloveEmbedding(glove_path):\n",
    "    word_vec = {}\n",
    "\n",
    "    word_vec[\"<PAD>\"] = np.zeros(101)  \n",
    "    with open(glove_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            values = line.strip().split()\n",
    "            word = values[0]\n",
    "            vector = np.array(values[1:], dtype=np.float32)\n",
    "            if word.lower() == word.capitalize():\n",
    "                lower_vector = np.append(vector, 0)\n",
    "                word_vec[word.lower()] = lower_vector\n",
    "            else:\n",
    "                lower_vector = np.append(vector, 0)\n",
    "                upper_vector = np.append(vector, 1)\n",
    "                word_vec[word.lower()] = lower_vector\n",
    "                word_vec[word.capitalize()] = upper_vector\n",
    "\n",
    "    \n",
    "    word_vec[\"<UNK>\"] = np.ones(101)\n",
    "\n",
    "    return word_vec\n",
    "\n",
    "\n",
    "def mapWordToGloveIndex(word_vec):\n",
    "    rev_map = {word: index for index, word in enumerate(word_vec)}\n",
    "    return rev_map\n",
    "\n",
    "\n",
    "def getEmbeddingLayerWeights(word_vec, word_index_map):\n",
    "    vocab_size = len(word_index_map) + 2\n",
    "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "    for word, idx in word_index_map.items():\n",
    "        if word in word_vec:\n",
    "            embedding_matrix[idx] = word_vec[word]\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vec = readGloveEmbedding(glove_file)\n",
    "\n",
    "word_index_map = mapWordToGloveIndex(word_vec)\n",
    "\n",
    "embedding_layer_weights = getEmbeddingLayerWeights(word_vec, word_index_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLSTM(nn.Module):\n",
    "  def __init__(self, num_classes):\n",
    "    super(CustomLSTM, self).__init__()\n",
    "    self.embedding_dim = embedding_dim\n",
    "    self.hidden_dim = 256\n",
    "    self.lstm_dropout = 0.33\n",
    "    self.linear_dim = 128\n",
    "\n",
    "    self.embedding_layer = nn.Embedding.from_pretrained(\n",
    "      embeddings=torch.FloatTensor(embedding_layer_weights)\n",
    "    )\n",
    "\n",
    "    self.lstm_layer = nn.LSTM(\n",
    "      input_size=self.embedding_dim,\n",
    "      hidden_size=self.hidden_dim,\n",
    "      bidirectional=True,\n",
    "      batch_first=True\n",
    "    )\n",
    "\n",
    "    self.dropout_layer = nn.Dropout(self.lstm_dropout)\n",
    "\n",
    "    self.linear_layer = nn.Linear(\n",
    "      in_features=self.hidden_dim * 2,\n",
    "      out_features=self.linear_dim\n",
    "    )\n",
    "\n",
    "    self.elu_layer = nn.ELU()\n",
    "\n",
    "    self.classifier = nn.Linear(\n",
    "      in_features=self.linear_dim,\n",
    "      out_features=num_classes,\n",
    "    )\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.embedding_layer(x)\n",
    "    x, _ = self.lstm_layer(x)\n",
    "    x = self.dropout_layer(x)\n",
    "    x = self.linear_layer(x)\n",
    "    x = self.elu_layer(x)\n",
    "    x = self.classifier(x)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_lstm_model(dataloader, num_classes, weight_balance):\n",
    "  model = CustomLSTM(num_classes)\n",
    "\n",
    "  learning_rate = 0.095\n",
    "  criterion = nn.CrossEntropyLoss(ignore_index=padding_value, weight=weight_balance)\n",
    "  optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "  exp_scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.95)\n",
    "  no_epochs = 40\n",
    "\n",
    "  for epoch in range(no_epochs):\n",
    "    train_loss = 0.0\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    match = 0\n",
    "    word_count = 0\n",
    "\n",
    "    for data, target in dataloader:\n",
    "      optimizer.zero_grad()\n",
    "\n",
    "      output = model(data)\n",
    "\n",
    "      output = output.view(-1, num_classes)\n",
    "      target = target.view(-1)\n",
    "\n",
    "      for i in range(output.shape[0]):\n",
    "        pred = torch.argmax(output[i])\n",
    "        actual = target[i].item()\n",
    "        word_count += 1\n",
    "        if (actual == padding_value):\n",
    "          continue\n",
    "        if (pred == actual):\n",
    "          match += 1\n",
    "\n",
    "      loss = criterion(output, target)\n",
    "\n",
    "      loss.backward()\n",
    "\n",
    "      optimizer.step()\n",
    "\n",
    "      train_loss += loss.item()\n",
    "\n",
    "    train_loss = train_loss/len(dataloader.dataset)\n",
    "\n",
    "    exp_scheduler.step()\n",
    "    \n",
    "    accuracy = (match / word_count) * 100\n",
    "\n",
    "    print(\n",
    "      'Epoch: {} \\tTraining Loss: {:.6f}'.format(\n",
    "        epoch+1,\n",
    "        train_loss,\n",
    "      )\n",
    "    )\n",
    "    print(\"Accuracy: \", accuracy)\n",
    "\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "  def __init__(self, X, Y):\n",
    "    self.X = X\n",
    "    self.Y = Y\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.X)\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "    return self.X[index], self.Y[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate_fn(batch):\n",
    "    sentences = [torch.LongTensor(item[0]) for item in batch]\n",
    "    tags = [torch.LongTensor(item[1]) for item in batch]\n",
    "\n",
    "    padded_sentences = pad_sequence(\n",
    "        sentences, batch_first=True)    \n",
    "    padded_tags = pad_sequence(\n",
    "        tags, batch_first=True, padding_value=padding_value)\n",
    "\n",
    "    return padded_sentences, padded_tags\n",
    "\n",
    "\n",
    "def getDatasetAndDataLoader(X, Y, batch_size=32):\n",
    "    dataset = CustomDataset(X, Y)\n",
    "\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size,\n",
    "                            shuffle=False, collate_fn=custom_collate_fn)\n",
    "\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentences = readFile(train_file)\n",
    "\n",
    "train_sentences = sorted(train_sentences, key=lambda x: len(x))\n",
    "\n",
    "nerTagSet = set()\n",
    "nerTagFreq = {}\n",
    "for sentence in train_sentences:\n",
    "  for word, nerTag in sentence:\n",
    "    nerTagSet.add(nerTag)\n",
    "    if nerTag in nerTagFreq:\n",
    "      nerTagFreq[nerTag] += 1\n",
    "    else:\n",
    "      nerTagFreq[nerTag] = 1\n",
    "  \n",
    "tag_map = { tag: index for index, tag in enumerate(list(nerTagSet)) }\n",
    "\n",
    "map_index_to_tag = {\n",
    "  index: tag for tag, index in tag_map.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "Y = []\n",
    "\n",
    "for sentence in train_sentences:\n",
    "  ip = []\n",
    "  op = []\n",
    "\n",
    "  for word, tag in sentence:\n",
    "    ip.append(word_index_map.get(word, unknown_value))  \n",
    "    op.append(tag_map[tag])\n",
    "\n",
    "  X.append(ip)\n",
    "  Y.append(op)\n",
    "\n",
    "train_dataloader = getDatasetAndDataLoader(X, Y, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.4496, 0.0098, 0.2634, 0.4844, 1.4393, 1.4418, 0.2523, 0.3678, 0.2332])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = [0 for _ in range(len(tag_map))]\n",
    "\n",
    "for index, tag in enumerate(tag_map):\n",
    "  weights[index] = len(train_sentences) / (len(tag_map) * nerTagFreq[tag])\n",
    "\n",
    "weight_balance = torch.Tensor(weights)\n",
    "\n",
    "weight_balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.116681\n",
      "Accuracy:  83.43901165146168\n",
      "Epoch: 2 \tTraining Loss: 0.072189\n",
      "Accuracy:  88.48862920821801\n",
      "Epoch: 3 \tTraining Loss: 0.062164\n",
      "Accuracy:  89.65670410559045\n",
      "Epoch: 4 \tTraining Loss: 0.056598\n",
      "Accuracy:  90.35715854676448\n",
      "Epoch: 5 \tTraining Loss: 0.052015\n",
      "Accuracy:  90.97658480868076\n",
      "Epoch: 6 \tTraining Loss: 0.048789\n",
      "Accuracy:  91.32217487443073\n",
      "Epoch: 7 \tTraining Loss: 0.045626\n",
      "Accuracy:  91.85813234928027\n",
      "Epoch: 8 \tTraining Loss: 0.043317\n",
      "Accuracy:  92.09877627924459\n",
      "Epoch: 9 \tTraining Loss: 0.040961\n",
      "Accuracy:  92.39213733788263\n",
      "Epoch: 10 \tTraining Loss: 0.038883\n",
      "Accuracy:  92.59861275852138\n",
      "Epoch: 11 \tTraining Loss: 0.037600\n",
      "Accuracy:  92.84853099815979\n",
      "Epoch: 12 \tTraining Loss: 0.035881\n",
      "Accuracy:  93.06379260691082\n",
      "Epoch: 13 \tTraining Loss: 0.034964\n",
      "Accuracy:  93.16873874269648\n",
      "Epoch: 14 \tTraining Loss: 0.033356\n",
      "Accuracy:  93.48113654224449\n",
      "Epoch: 15 \tTraining Loss: 0.032832\n",
      "Accuracy:  93.59242825833346\n",
      "Epoch: 16 \tTraining Loss: 0.031351\n",
      "Accuracy:  93.8399058901629\n",
      "Epoch: 17 \tTraining Loss: 0.030835\n",
      "Accuracy:  93.86089511732003\n",
      "Epoch: 18 \tTraining Loss: 0.030150\n",
      "Accuracy:  93.97755617058873\n",
      "Epoch: 19 \tTraining Loss: 0.029197\n",
      "Accuracy:  94.0654180517116\n",
      "Epoch: 20 \tTraining Loss: 0.028980\n",
      "Accuracy:  94.19525838714874\n",
      "Epoch: 21 \tTraining Loss: 0.028135\n",
      "Accuracy:  94.39099513342802\n",
      "Epoch: 22 \tTraining Loss: 0.027852\n",
      "Accuracy:  94.44322414053997\n",
      "Epoch: 23 \tTraining Loss: 0.027659\n",
      "Accuracy:  94.47544016361834\n",
      "Epoch: 24 \tTraining Loss: 0.027082\n",
      "Accuracy:  94.59405370313422\n",
      "Epoch: 25 \tTraining Loss: 0.026614\n",
      "Accuracy:  94.59942304031395\n",
      "Epoch: 26 \tTraining Loss: 0.026010\n",
      "Accuracy:  94.67947497644813\n",
      "Epoch: 27 \tTraining Loss: 0.025921\n",
      "Accuracy:  94.67117690989764\n",
      "Epoch: 28 \tTraining Loss: 0.025617\n",
      "Accuracy:  94.7995528806494\n",
      "Epoch: 29 \tTraining Loss: 0.025330\n",
      "Accuracy:  94.81712525687396\n",
      "Epoch: 30 \tTraining Loss: 0.024769\n",
      "Accuracy:  94.92548824359218\n",
      "Epoch: 31 \tTraining Loss: 0.024805\n",
      "Accuracy:  94.9713716704008\n",
      "Epoch: 32 \tTraining Loss: 0.024634\n",
      "Accuracy:  94.99577774849048\n",
      "Epoch: 33 \tTraining Loss: 0.024578\n",
      "Accuracy:  95.01286200315326\n",
      "Epoch: 34 \tTraining Loss: 0.024509\n",
      "Accuracy:  95.03287498718682\n",
      "Epoch: 35 \tTraining Loss: 0.024139\n",
      "Accuracy:  95.14855979733193\n",
      "Epoch: 36 \tTraining Loss: 0.024080\n",
      "Accuracy:  95.11390316644457\n",
      "Epoch: 37 \tTraining Loss: 0.023829\n",
      "Accuracy:  95.16515593043292\n",
      "Epoch: 38 \tTraining Loss: 0.023925\n",
      "Accuracy:  95.21884930223023\n",
      "Epoch: 39 \tTraining Loss: 0.023889\n",
      "Accuracy:  95.2212899100392\n",
      "Epoch: 40 \tTraining Loss: 0.023216\n",
      "Accuracy:  95.24667223125248\n"
     ]
    }
   ],
   "source": [
    "model = train_lstm_model(dataloader=train_dataloader, num_classes=len(tag_map), weight_balance=torch.Tensor(weight_balance))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, \"blstm2.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_dev(pred_model, dataloader):\n",
    "  # pred_model.eval()\n",
    "  y_actual = []\n",
    "  y_pred = []\n",
    "  \n",
    "  # with torch.no_grad():\n",
    "  for data, target in dataloader:\n",
    "    pred = pred_model(data)\n",
    "\n",
    "    pred = torch.argmax(pred, 2)\n",
    "\n",
    "    y_pred.extend(pred)\n",
    "    y_actual.extend(target)\n",
    "\n",
    "  return y_pred, y_actual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_sentences = readFile(dev_file)\n",
    "\n",
    "X_dev = []\n",
    "Y_dev = []\n",
    "\n",
    "for sentence in dev_sentences:\n",
    "  ip_dev = []\n",
    "  op_dev = []\n",
    "\n",
    "  for word, tag in sentence:\n",
    "    ip_dev.append(word_index_map.get(word, unknown_value))\n",
    "    op_dev.append(tag_map[tag])\n",
    "\n",
    "  X_dev.append(ip_dev)\n",
    "  Y_dev.append(op_dev)\n",
    "\n",
    "dev_dataloader = getDatasetAndDataLoader(X_dev, Y_dev, batch_size=2)\n",
    "\n",
    "y_pred, y_actual = predict_dev(model, dev_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_sentences = []\n",
    "for index, sentence in enumerate(dev_sentences):\n",
    "  preds = y_pred[index].tolist()\n",
    "  if not isinstance(preds, list):\n",
    "    preds = [preds]\n",
    "  curr_sentence = []\n",
    "  for word_index, (word, _) in enumerate(sentence):\n",
    "    curr_sentence.append((word, map_index_to_tag[preds[word_index]]))\n",
    "  pred_sentences.append(curr_sentence)\n",
    "\n",
    "writeFile(\"dev2.out\", pred_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestDataset(Dataset):\n",
    "    def __init__(self, X):\n",
    "        self.X = X\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.X[index]\n",
    "\n",
    "\n",
    "def test_collate_fn(batch):\n",
    "    sentences = [torch.LongTensor(item) for item in batch]\n",
    "\n",
    "    padded_sentences = pad_sequence(\n",
    "        sentences, batch_first=True)\n",
    "\n",
    "    return padded_sentences\n",
    "\n",
    "\n",
    "def getTestDataloader(X, batch_size=2):\n",
    "    dataset = TestDataset(X)\n",
    "\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=False, collate_fn=test_collate_fn)\n",
    "\n",
    "\n",
    "def predict_test(pred_model, dataloader):\n",
    "    # pred_model.eval()\n",
    "    y_pred = []\n",
    "\n",
    "    # with torch.no_grad():\n",
    "    for data in dataloader:\n",
    "        pred = pred_model(data)\n",
    "\n",
    "        pred = torch.argmax(pred, 2)\n",
    "\n",
    "        y_pred.extend(pred)\n",
    "\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentences = readFile(test_file, entity=False)\n",
    "\n",
    "X_test = []\n",
    "\n",
    "for sentence in test_sentences:\n",
    "  ip_test = []\n",
    "  for word in sentence:\n",
    "    ip_test.append(word_index_map.get(word, unknown_value))\n",
    "  X_test.append(ip_test)\n",
    "\n",
    "test_dataloader = getTestDataloader(X_test)\n",
    "\n",
    "y_pred = predict_test(model, test_dataloader)\n",
    "\n",
    "pred_sentences = []\n",
    "for index, sentence in enumerate(test_sentences):\n",
    "  preds = y_pred[index].tolist()\n",
    "  if not isinstance(preds, list):\n",
    "    preds = [preds]\n",
    "  curr_sentence = []\n",
    "  for word_index, word in enumerate(sentence):\n",
    "    curr_sentence.append((word, map_index_to_tag[preds[word_index]]))\n",
    "  pred_sentences.append(curr_sentence)\n",
    "\n",
    "writeFile(\"test2.out\", pred_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "processed 51578 tokens with 5942 phrases; found: 7693 phrases; correct: 4697.\n",
    "\n",
    "- accuracy:  93.40%; precision:  61.06%; recall:  79.05%; FB1:  68.90\n",
    "\n",
    "- LOC: precision:  66.88%; recall:  78.61%; FB1:  72.27  2159\n",
    "\n",
    "- MISC: precision:  41.42%; recall:  67.57%; FB1:  51.36  1504\n",
    "\n",
    "- ORG: precision:  49.06%; recall:  72.04%; FB1:  58.37  1969\n",
    "\n",
    "- PER: precision:  80.74%; recall:  90.34%; FB1:  85.27  2061"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
